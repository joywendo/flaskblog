* 
* ==> Audit <==
* |-----------|--------------------------------|----------|------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|------|---------|---------------------|---------------------|
| start     | --driver=docker                | minikube | joy  | v1.32.0 | 25 Jan 24 16:54 EAT | 25 Jan 24 17:19 EAT |
| dashboard |                                | minikube | joy  | v1.32.0 | 25 Jan 24 17:21 EAT |                     |
| dashboard |                                | minikube | joy  | v1.32.0 | 25 Jan 24 18:54 EAT |                     |
| start     |                                | minikube | joy  | v1.32.0 | 28 Jan 24 16:47 EAT | 28 Jan 24 16:49 EAT |
| image     | load flask_web:latest          | minikube | joy  | v1.32.0 | 28 Jan 24 16:52 EAT |                     |
| image     | load flask_web:latest          | minikube | joy  | v1.32.0 | 28 Jan 24 17:15 EAT |                     |
| image     | load flask_web:latest          | minikube | joy  | v1.32.0 | 28 Jan 24 17:17 EAT | 28 Jan 24 17:17 EAT |
|           | /home/joy/Desktop/flask        |          |      |         |                     |                     |
| image     | load flask_web:latest          | minikube | joy  | v1.32.0 | 28 Jan 24 17:17 EAT |                     |
| cache     | delete your-image-name:tag     | minikube | joy  | v1.32.0 | 28 Jan 24 17:20 EAT |                     |
|-----------|--------------------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/28 16:47:38
Running on machine: joy-Swift-SF314-51
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0128 16:47:38.931001    8155 out.go:296] Setting OutFile to fd 1 ...
I0128 16:47:38.931473    8155 out.go:348] isatty.IsTerminal(1) = true
I0128 16:47:38.931486    8155 out.go:309] Setting ErrFile to fd 2...
I0128 16:47:38.931506    8155 out.go:348] isatty.IsTerminal(2) = true
I0128 16:47:38.932577    8155 root.go:338] Updating PATH: /home/joy/.minikube/bin
W0128 16:47:38.933514    8155 root.go:314] Error reading config file at /home/joy/.minikube/config/config.json: open /home/joy/.minikube/config/config.json: no such file or directory
I0128 16:47:38.937712    8155 out.go:303] Setting JSON to false
I0128 16:47:38.944911    8155 start.go:128] hostinfo: {"hostname":"joy-Swift-SF314-51","uptime":3669,"bootTime":1706445990,"procs":260,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-15-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"8339c05a-a79f-47b0-97c9-dbaaeb2f72e5"}
I0128 16:47:38.945138    8155 start.go:138] virtualization: kvm host
I0128 16:47:38.953575    8155 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0128 16:47:38.959319    8155 notify.go:220] Checking for updates...
I0128 16:47:38.963537    8155 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0128 16:47:38.971053    8155 driver.go:378] Setting default libvirt URI to qemu:///system
I0128 16:47:39.876987    8155 docker.go:122] docker version: linux-25.0.1:Docker Engine - Community
I0128 16:47:39.877271    8155 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 16:47:42.909979    8155 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.032637594s)
I0128 16:47:42.912716    8155 info.go:266] docker info: {ID:4db24b99-dc9c-4078-bff8-b13766894bb1 Containers:13 ContainersRunning:1 ContainersPaused:0 ContainersStopped:12 Images:21 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:30 OomKillDisable:false NGoroutines:51 SystemTime:2024-01-28 16:47:42.866369703 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-15-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8049299456 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:joy-Swift-SF314-51 Labels:[] ExperimentalBuild:false ServerVersion:25.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:a1496014c916f9e62104b33d1bb5bd03b0858e59 Expected:a1496014c916f9e62104b33d1bb5bd03b0858e59} RuncCommit:{ID:v1.1.11-0-g4bccb38 Expected:v1.1.11-0-g4bccb38} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.2]] Warnings:<nil>}}
I0128 16:47:42.913375    8155 docker.go:295] overlay module found
I0128 16:47:42.918695    8155 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0128 16:47:42.921316    8155 start.go:298] selected driver: docker
I0128 16:47:42.921369    8155 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0128 16:47:42.921538    8155 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0128 16:47:42.921792    8155 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 16:47:43.233681    8155 info.go:266] docker info: {ID:4db24b99-dc9c-4078-bff8-b13766894bb1 Containers:13 ContainersRunning:1 ContainersPaused:0 ContainersStopped:12 Images:21 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:30 OomKillDisable:false NGoroutines:51 SystemTime:2024-01-28 16:47:43.196911802 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-15-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8049299456 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:joy-Swift-SF314-51 Labels:[] ExperimentalBuild:false ServerVersion:25.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:a1496014c916f9e62104b33d1bb5bd03b0858e59 Expected:a1496014c916f9e62104b33d1bb5bd03b0858e59} RuncCommit:{ID:v1.1.11-0-g4bccb38 Expected:v1.1.11-0-g4bccb38} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.2]] Warnings:<nil>}}
I0128 16:47:43.235987    8155 cni.go:84] Creating CNI manager for ""
I0128 16:47:43.236052    8155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 16:47:43.236104    8155 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0128 16:47:43.243191    8155 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0128 16:47:43.245962    8155 cache.go:121] Beginning downloading kic base image for docker with docker
I0128 16:47:43.248751    8155 out.go:177] üöú  Pulling base image ...
I0128 16:47:43.253184    8155 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42 in local docker daemon
I0128 16:47:43.253867    8155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0128 16:47:43.254191    8155 preload.go:148] Found local preload: /home/joy/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0128 16:47:43.254242    8155 cache.go:56] Caching tarball of preloaded images
I0128 16:47:43.254906    8155 preload.go:174] Found /home/joy/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0128 16:47:43.254972    8155 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0128 16:47:43.255486    8155 profile.go:148] Saving config to /home/joy/.minikube/profiles/minikube/config.json ...
I0128 16:47:43.335207    8155 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42 in local docker daemon, skipping pull
I0128 16:47:43.335242    8155 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42 exists in daemon, skipping load
I0128 16:47:43.335285    8155 cache.go:194] Successfully downloaded all kic artifacts
I0128 16:47:43.335365    8155 start.go:365] acquiring machines lock for minikube: {Name:mk0f3d4bb3868b65aeaef8dfb6f566fdfc3323ff Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0128 16:47:43.335635    8155 start.go:369] acquired machines lock for "minikube" in 218.736¬µs
I0128 16:47:43.335684    8155 start.go:96] Skipping create...Using existing machine configuration
I0128 16:47:43.335700    8155 fix.go:54] fixHost starting: 
I0128 16:47:43.337202    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:47:43.402687    8155 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0128 16:47:43.402726    8155 fix.go:128] unexpected machine state, will restart: <nil>
I0128 16:47:43.408832    8155 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0128 16:47:43.411575    8155 cli_runner.go:164] Run: docker start minikube
I0128 16:47:44.647147    8155 cli_runner.go:217] Completed: docker start minikube: (1.235502833s)
I0128 16:47:44.647355    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:47:44.716931    8155 kic.go:430] container "minikube" state is running.
I0128 16:47:44.718005    8155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 16:47:44.790990    8155 profile.go:148] Saving config to /home/joy/.minikube/profiles/minikube/config.json ...
I0128 16:47:44.791571    8155 machine.go:88] provisioning docker machine ...
I0128 16:47:44.791699    8155 ubuntu.go:169] provisioning hostname "minikube"
I0128 16:47:44.791874    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:44.862537    8155 main.go:141] libmachine: Using SSH client type: native
I0128 16:47:44.864224    8155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 16:47:44.864252    8155 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0128 16:47:44.866258    8155 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:60170->127.0.0.1:32772: read: connection reset by peer
I0128 16:47:47.867598    8155 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:60180->127.0.0.1:32772: read: connection reset by peer
I0128 16:47:51.611462    8155 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 16:47:51.611746    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:51.679697    8155 main.go:141] libmachine: Using SSH client type: native
I0128 16:47:51.681596    8155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 16:47:51.681745    8155 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0128 16:47:51.989464    8155 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 16:47:51.989565    8155 ubuntu.go:175] set auth options {CertDir:/home/joy/.minikube CaCertPath:/home/joy/.minikube/certs/ca.pem CaPrivateKeyPath:/home/joy/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/joy/.minikube/machines/server.pem ServerKeyPath:/home/joy/.minikube/machines/server-key.pem ClientKeyPath:/home/joy/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/joy/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/joy/.minikube}
I0128 16:47:51.989678    8155 ubuntu.go:177] setting up certificates
I0128 16:47:51.989711    8155 provision.go:83] configureAuth start
I0128 16:47:51.989909    8155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 16:47:52.057539    8155 provision.go:138] copyHostCerts
I0128 16:47:52.059392    8155 exec_runner.go:144] found /home/joy/.minikube/ca.pem, removing ...
I0128 16:47:52.059424    8155 exec_runner.go:203] rm: /home/joy/.minikube/ca.pem
I0128 16:47:52.059595    8155 exec_runner.go:151] cp: /home/joy/.minikube/certs/ca.pem --> /home/joy/.minikube/ca.pem (1070 bytes)
I0128 16:47:52.062461    8155 exec_runner.go:144] found /home/joy/.minikube/cert.pem, removing ...
I0128 16:47:52.062486    8155 exec_runner.go:203] rm: /home/joy/.minikube/cert.pem
I0128 16:47:52.062656    8155 exec_runner.go:151] cp: /home/joy/.minikube/certs/cert.pem --> /home/joy/.minikube/cert.pem (1115 bytes)
I0128 16:47:52.071088    8155 exec_runner.go:144] found /home/joy/.minikube/key.pem, removing ...
I0128 16:47:52.071119    8155 exec_runner.go:203] rm: /home/joy/.minikube/key.pem
I0128 16:47:52.071282    8155 exec_runner.go:151] cp: /home/joy/.minikube/certs/key.pem --> /home/joy/.minikube/key.pem (1679 bytes)
I0128 16:47:52.075276    8155 provision.go:112] generating server cert: /home/joy/.minikube/machines/server.pem ca-key=/home/joy/.minikube/certs/ca.pem private-key=/home/joy/.minikube/certs/ca-key.pem org=joy.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0128 16:47:53.599854    8155 provision.go:172] copyRemoteCerts
I0128 16:47:53.600120    8155 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0128 16:47:53.600233    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:53.690200    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:47:53.949912    8155 ssh_runner.go:362] scp /home/joy/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0128 16:47:54.138554    8155 ssh_runner.go:362] scp /home/joy/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0128 16:47:54.282745    8155 ssh_runner.go:362] scp /home/joy/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0128 16:47:54.468912    8155 provision.go:86] duration metric: configureAuth took 2.479171587s
I0128 16:47:54.468957    8155 ubuntu.go:193] setting minikube options for container-runtime
I0128 16:47:54.469523    8155 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0128 16:47:54.469680    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:54.557799    8155 main.go:141] libmachine: Using SSH client type: native
I0128 16:47:54.559709    8155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 16:47:54.559934    8155 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0128 16:47:54.878353    8155 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0128 16:47:54.878388    8155 ubuntu.go:71] root file system type: overlay
I0128 16:47:54.878785    8155 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0128 16:47:54.878995    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:54.947773    8155 main.go:141] libmachine: Using SSH client type: native
I0128 16:47:54.948937    8155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 16:47:54.949129    8155 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0128 16:47:55.255815    8155 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0128 16:47:55.256018    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:55.320354    8155 main.go:141] libmachine: Using SSH client type: native
I0128 16:47:55.321440    8155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 16:47:55.321483    8155 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0128 16:47:55.630231    8155 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 16:47:55.630267    8155 machine.go:91] provisioned docker machine in 10.838676055s
I0128 16:47:55.630289    8155 start.go:300] post-start starting for "minikube" (driver="docker")
I0128 16:47:55.630318    8155 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0128 16:47:55.630483    8155 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0128 16:47:55.630639    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:55.706593    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:47:55.888670    8155 ssh_runner.go:195] Run: cat /etc/os-release
I0128 16:47:55.900826    8155 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0128 16:47:55.900887    8155 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0128 16:47:55.900912    8155 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0128 16:47:55.900924    8155 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0128 16:47:55.900944    8155 filesync.go:126] Scanning /home/joy/.minikube/addons for local assets ...
I0128 16:47:55.901580    8155 filesync.go:126] Scanning /home/joy/.minikube/files for local assets ...
I0128 16:47:55.902064    8155 start.go:303] post-start completed in 271.757256ms
I0128 16:47:55.902242    8155 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0128 16:47:55.902334    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:55.971303    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:47:56.162050    8155 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0128 16:47:56.192843    8155 fix.go:56] fixHost completed within 12.857126528s
I0128 16:47:56.192882    8155 start.go:83] releasing machines lock for "minikube", held for 12.85721957s
I0128 16:47:56.193092    8155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 16:47:56.253514    8155 ssh_runner.go:195] Run: cat /version.json
I0128 16:47:56.253651    8155 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0128 16:47:56.253661    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:56.253802    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:47:56.314587    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:47:56.318431    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:47:58.039577    8155 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.785867327s)
I0128 16:47:58.039799    8155 ssh_runner.go:235] Completed: cat /version.json: (1.786234651s)
I0128 16:47:58.040108    8155 ssh_runner.go:195] Run: systemctl --version
I0128 16:47:58.083777    8155 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0128 16:47:58.101013    8155 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0128 16:47:58.183882    8155 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0128 16:47:58.184060    8155 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0128 16:47:58.219955    8155 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0128 16:47:58.219986    8155 start.go:472] detecting cgroup driver to use...
I0128 16:47:58.220053    8155 detect.go:199] detected "systemd" cgroup driver on host os
I0128 16:47:58.220265    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 16:47:58.293072    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0128 16:47:58.331672    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0128 16:47:58.373778    8155 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0128 16:47:58.373920    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0128 16:47:58.419371    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 16:47:58.466773    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0128 16:47:58.510742    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 16:47:58.566265    8155 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0128 16:47:58.608125    8155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0128 16:47:58.649100    8155 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0128 16:47:58.693619    8155 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0128 16:47:58.752538    8155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 16:47:59.163255    8155 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0128 16:47:59.475292    8155 start.go:472] detecting cgroup driver to use...
I0128 16:47:59.475379    8155 detect.go:199] detected "systemd" cgroup driver on host os
I0128 16:47:59.475531    8155 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0128 16:47:59.570883    8155 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0128 16:47:59.571068    8155 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0128 16:47:59.633888    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 16:47:59.727384    8155 ssh_runner.go:195] Run: which cri-dockerd
I0128 16:47:59.751636    8155 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0128 16:47:59.797796    8155 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0128 16:47:59.871498    8155 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0128 16:48:00.282924    8155 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0128 16:48:00.819209    8155 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0128 16:48:00.819542    8155 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0128 16:48:00.908015    8155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 16:48:01.371760    8155 ssh_runner.go:195] Run: sudo systemctl restart docker
I0128 16:48:03.210743    8155 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.838914357s)
I0128 16:48:03.210929    8155 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 16:48:03.761445    8155 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0128 16:48:04.261410    8155 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 16:48:04.677191    8155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 16:48:05.203201    8155 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0128 16:48:05.283597    8155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 16:48:05.709466    8155 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0128 16:48:07.577416    8155 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (1.867890851s)
I0128 16:48:07.577456    8155 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0128 16:48:07.578005    8155 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0128 16:48:07.606563    8155 start.go:540] Will wait 60s for crictl version
I0128 16:48:07.606725    8155 ssh_runner.go:195] Run: which crictl
I0128 16:48:07.625052    8155 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0128 16:48:08.869107    8155 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.243998241s)
I0128 16:48:08.869147    8155 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0128 16:48:08.869291    8155 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 16:48:09.785596    8155 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 16:48:09.898879    8155 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0128 16:48:09.899195    8155 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0128 16:48:09.976036    8155 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0128 16:48:09.992377    8155 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 16:48:10.048579    8155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0128 16:48:10.048793    8155 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 16:48:10.143093    8155 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 16:48:10.143132    8155 docker.go:601] Images already preloaded, skipping extraction
I0128 16:48:10.143349    8155 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 16:48:10.232512    8155 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 16:48:10.232557    8155 cache_images.go:84] Images are preloaded, skipping loading
I0128 16:48:10.232728    8155 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0128 16:48:12.082039    8155 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.849241935s)
I0128 16:48:12.082230    8155 cni.go:84] Creating CNI manager for ""
I0128 16:48:12.082278    8155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 16:48:12.085930    8155 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0128 16:48:12.086073    8155 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0128 16:48:12.086627    8155 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0128 16:48:12.086881    8155 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0128 16:48:12.087060    8155 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0128 16:48:12.137853    8155 binaries.go:44] Found k8s binaries, skipping transfer
I0128 16:48:12.138086    8155 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0128 16:48:12.182337    8155 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0128 16:48:12.279147    8155 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0128 16:48:12.376278    8155 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0128 16:48:12.491962    8155 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0128 16:48:12.509698    8155 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 16:48:12.587792    8155 certs.go:56] Setting up /home/joy/.minikube/profiles/minikube for IP: 192.168.49.2
I0128 16:48:12.587865    8155 certs.go:190] acquiring lock for shared ca certs: {Name:mk83cc8c4e5328f3633cba6b74d03e35e8fda1b6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 16:48:12.589209    8155 certs.go:199] skipping minikubeCA CA generation: /home/joy/.minikube/ca.key
I0128 16:48:12.590063    8155 certs.go:199] skipping proxyClientCA CA generation: /home/joy/.minikube/proxy-client-ca.key
I0128 16:48:12.591077    8155 certs.go:315] skipping minikube-user signed cert generation: /home/joy/.minikube/profiles/minikube/client.key
I0128 16:48:12.591855    8155 certs.go:315] skipping minikube signed cert generation: /home/joy/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0128 16:48:12.592680    8155 certs.go:315] skipping aggregator signed cert generation: /home/joy/.minikube/profiles/minikube/proxy-client.key
I0128 16:48:12.593409    8155 certs.go:437] found cert: /home/joy/.minikube/certs/home/joy/.minikube/certs/ca-key.pem (1675 bytes)
I0128 16:48:12.593685    8155 certs.go:437] found cert: /home/joy/.minikube/certs/home/joy/.minikube/certs/ca.pem (1070 bytes)
I0128 16:48:12.593837    8155 certs.go:437] found cert: /home/joy/.minikube/certs/home/joy/.minikube/certs/cert.pem (1115 bytes)
I0128 16:48:12.593974    8155 certs.go:437] found cert: /home/joy/.minikube/certs/home/joy/.minikube/certs/key.pem (1679 bytes)
I0128 16:48:12.597561    8155 ssh_runner.go:362] scp /home/joy/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0128 16:48:12.778159    8155 ssh_runner.go:362] scp /home/joy/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0128 16:48:12.946442    8155 ssh_runner.go:362] scp /home/joy/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0128 16:48:13.106355    8155 ssh_runner.go:362] scp /home/joy/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0128 16:48:13.294859    8155 ssh_runner.go:362] scp /home/joy/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0128 16:48:13.468214    8155 ssh_runner.go:362] scp /home/joy/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0128 16:48:13.606306    8155 ssh_runner.go:362] scp /home/joy/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0128 16:48:13.799115    8155 ssh_runner.go:362] scp /home/joy/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0128 16:48:13.946820    8155 ssh_runner.go:362] scp /home/joy/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0128 16:48:14.123947    8155 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0128 16:48:14.219115    8155 ssh_runner.go:195] Run: openssl version
I0128 16:48:14.271631    8155 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0128 16:48:14.321224    8155 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0128 16:48:14.337820    8155 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan 25 14:18 /usr/share/ca-certificates/minikubeCA.pem
I0128 16:48:14.337971    8155 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0128 16:48:14.369748    8155 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0128 16:48:14.419443    8155 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0128 16:48:14.441559    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0128 16:48:14.480529    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0128 16:48:14.518251    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0128 16:48:14.553434    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0128 16:48:14.590340    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0128 16:48:14.629230    8155 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0128 16:48:14.667752    8155 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0128 16:48:14.668113    8155 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 16:48:14.774756    8155 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0128 16:48:14.834455    8155 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0128 16:48:14.834618    8155 kubeadm.go:636] restartCluster start
I0128 16:48:14.834842    8155 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0128 16:48:14.881983    8155 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0128 16:48:14.892420    8155 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0128 16:48:15.007454    8155 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0128 16:48:15.059165    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:15.059311    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:15.116685    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:15.116709    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:15.116849    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:15.180770    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:15.680849    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:15.681064    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:15.740673    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:16.181878    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:16.182073    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:16.236249    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:16.682712    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:16.682874    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:16.752050    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:17.181484    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:17.181638    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:17.238477    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:17.685083    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:17.685332    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:17.755168    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:18.181517    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:18.181703    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:18.233271    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:18.687259    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:18.687459    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:18.774536    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:19.181751    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:19.181926    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:19.236708    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:19.683133    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:19.683303    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:19.744969    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:20.181499    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:20.181641    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:20.223248    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:20.681895    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:20.682056    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:20.724733    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:21.181166    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:21.181354    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:21.223938    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:21.681252    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:21.681436    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:21.726622    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:22.181256    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:22.181444    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:22.224894    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:22.681703    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:22.682104    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:22.740632    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:23.180863    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:23.181077    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:23.233774    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:23.681306    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:23.681540    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:23.734993    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:24.180903    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:24.181106    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:24.225243    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:24.681857    8155 api_server.go:166] Checking apiserver status ...
I0128 16:48:24.682060    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 16:48:24.740311    8155 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 16:48:25.059893    8155 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0128 16:48:25.059937    8155 kubeadm.go:1128] stopping kube-system containers ...
I0128 16:48:25.060187    8155 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 16:48:25.158617    8155 docker.go:469] Stopping containers: [171733a855a9 342d748b5430 fa6850b03965 86a19c6c1599 01ca9a71d75d 3616fa907c39 7a0c68f4c700 159ca0905797 d99a4c831f55 9045a5d0b388 74d4edc1bc7d f601c5a6c113 8fc077ebc747 18ef25765176 486fa507b401]
I0128 16:48:25.158831    8155 ssh_runner.go:195] Run: docker stop 171733a855a9 342d748b5430 fa6850b03965 86a19c6c1599 01ca9a71d75d 3616fa907c39 7a0c68f4c700 159ca0905797 d99a4c831f55 9045a5d0b388 74d4edc1bc7d f601c5a6c113 8fc077ebc747 18ef25765176 486fa507b401
I0128 16:48:25.256030    8155 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0128 16:48:25.323459    8155 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0128 16:48:25.386928    8155 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Jan 25 14:18 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jan 25 14:18 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 25 14:19 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Jan 25 14:18 /etc/kubernetes/scheduler.conf

I0128 16:48:25.387148    8155 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0128 16:48:25.444188    8155 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0128 16:48:25.512805    8155 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0128 16:48:25.585576    8155 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0128 16:48:25.585780    8155 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0128 16:48:25.639187    8155 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0128 16:48:25.702674    8155 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0128 16:48:25.702843    8155 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0128 16:48:25.761450    8155 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0128 16:48:25.823611    8155 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0128 16:48:25.823650    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:27.531009    8155 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.70729067s)
I0128 16:48:27.531068    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:29.896764    8155 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.36564782s)
I0128 16:48:29.896814    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:30.560433    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:30.877121    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:31.208616    8155 api_server.go:52] waiting for apiserver process to appear ...
I0128 16:48:31.208814    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:31.275110    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:31.842233    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:32.342024    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:32.841797    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:33.341395    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:33.841448    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:34.341125    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:34.841705    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:35.341479    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:35.842254    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:36.341575    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:36.841414    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:37.341584    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:37.841974    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:38.341152    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:48:38.386511    8155 api_server.go:72] duration metric: took 7.177866038s to wait for apiserver process to appear ...
I0128 16:48:38.386602    8155 api_server.go:88] waiting for apiserver healthz status ...
I0128 16:48:38.386676    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:38.387652    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:38.387712    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:38.388337    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:38.888710    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:38.889450    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:39.388539    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:39.389543    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:39.888794    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:39.889813    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:40.388724    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:40.389736    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:40.888637    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:40.889910    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:41.389498    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:41.390598    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:41.888730    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:41.889649    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:42.388894    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:42.389716    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:42.889420    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:42.890379    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:43.388716    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:43.389895    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:43.889351    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:43.890368    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:44.389371    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:44.390244    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:44.888789    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:44.889512    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:45.389039    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:45.389838    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:45.888669    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:45.889458    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0128 16:48:46.388969    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:51.389634    8155 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0128 16:48:51.389689    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:53.715020    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0128 16:48:53.715059    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 16:48:53.715091    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:53.761850    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0128 16:48:53.761893    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 16:48:53.888968    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:53.913092    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 16:48:53.913147    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 16:48:54.388467    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:54.404491    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 16:48:54.404553    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 16:48:54.888848    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:54.935392    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 16:48:54.935444    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 16:48:55.388833    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:55.417191    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 16:48:55.417240    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 16:48:55.888457    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:55.998416    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 16:48:55.998461    8155 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 16:48:56.389540    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:48:56.428655    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0128 16:48:56.500019    8155 api_server.go:141] control plane version: v1.28.3
I0128 16:48:56.500090    8155 api_server.go:131] duration metric: took 18.113443685s to wait for apiserver health ...
I0128 16:48:56.500117    8155 cni.go:84] Creating CNI manager for ""
I0128 16:48:56.500169    8155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 16:48:56.508279    8155 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0128 16:48:56.512305    8155 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0128 16:48:56.610893    8155 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0128 16:48:56.810813    8155 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 16:48:56.875792    8155 system_pods.go:59] 7 kube-system pods found
I0128 16:48:56.875877    8155 system_pods.go:61] "coredns-5dd5756b68-qtflq" [8ffd43dc-6f9b-4b14-93c5-401cc05acf60] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 16:48:56.875904    8155 system_pods.go:61] "etcd-minikube" [874fcf5a-5b38-4d76-8fdb-e7fa251d7bb2] Running
I0128 16:48:56.875946    8155 system_pods.go:61] "kube-apiserver-minikube" [e36fa7fe-1c12-4b7c-a5ee-d6e0e6407ccc] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0128 16:48:56.875996    8155 system_pods.go:61] "kube-controller-manager-minikube" [b855f90a-1e00-47ec-8880-f583dead332e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0128 16:48:56.876031    8155 system_pods.go:61] "kube-proxy-6w7wj" [37588c65-57ab-4237-9b2f-30ff380ae1ec] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0128 16:48:56.876054    8155 system_pods.go:61] "kube-scheduler-minikube" [3b6d5394-3ec1-4588-8584-d50c0b069bf4] Running
I0128 16:48:56.876084    8155 system_pods.go:61] "storage-provisioner" [a939272e-11d3-4775-bd6f-0e6573e7403a] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 16:48:56.876113    8155 system_pods.go:74] duration metric: took 65.264451ms to wait for pod list to return data ...
I0128 16:48:56.876136    8155 node_conditions.go:102] verifying NodePressure condition ...
I0128 16:48:56.904891    8155 node_conditions.go:122] node storage ephemeral capacity is 244505916Ki
I0128 16:48:56.904938    8155 node_conditions.go:123] node cpu capacity is 4
I0128 16:48:56.905003    8155 node_conditions.go:105] duration metric: took 28.853027ms to run NodePressure ...
I0128 16:48:56.905047    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0128 16:48:58.501731    8155 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.596602906s)
I0128 16:48:58.501788    8155 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0128 16:48:58.533108    8155 ops.go:34] apiserver oom_adj: -16
I0128 16:48:58.533132    8155 kubeadm.go:640] restartCluster took 43.698498665s
I0128 16:48:58.533152    8155 kubeadm.go:406] StartCluster complete in 43.865415625s
I0128 16:48:58.533195    8155 settings.go:142] acquiring lock: {Name:mk4799694790ff025da650b1a055e7b5f633a3fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 16:48:58.533510    8155 settings.go:150] Updating kubeconfig:  /home/joy/.kube/config
I0128 16:48:58.539635    8155 lock.go:35] WriteFile acquiring /home/joy/.kube/config: {Name:mk741ef515312c63947a90f393d69fbd9a32e19c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 16:48:58.540384    8155 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0128 16:48:58.540568    8155 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0128 16:48:58.540760    8155 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0128 16:48:58.540812    8155 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0128 16:48:58.540813    8155 addons.go:69] Setting dashboard=true in profile "minikube"
W0128 16:48:58.540834    8155 addons.go:240] addon storage-provisioner should already be in state true
I0128 16:48:58.540876    8155 addons.go:231] Setting addon dashboard=true in "minikube"
W0128 16:48:58.540897    8155 addons.go:240] addon dashboard should already be in state true
I0128 16:48:58.541160    8155 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0128 16:48:58.541228    8155 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0128 16:48:58.541329    8155 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0128 16:48:58.542590    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:48:58.550662    8155 host.go:66] Checking if "minikube" exists ...
I0128 16:48:58.552797    8155 host.go:66] Checking if "minikube" exists ...
I0128 16:48:58.555148    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:48:58.555765    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:48:58.569745    8155 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0128 16:48:58.569824    8155 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0128 16:48:58.602969    8155 out.go:177] üîé  Verifying Kubernetes components...
I0128 16:48:58.634848    8155 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0128 16:48:58.648826    8155 addons.go:240] addon default-storageclass should already be in state true
I0128 16:48:58.648912    8155 host.go:66] Checking if "minikube" exists ...
I0128 16:48:58.648693    8155 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0128 16:48:58.651100    8155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 16:48:58.743209    8155 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0128 16:48:58.755368    8155 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0128 16:48:58.811088    8155 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0128 16:48:58.845984    8155 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0128 16:48:58.846034    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0128 16:48:58.885123    8155 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0128 16:48:58.885153    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0128 16:48:58.885300    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:48:58.885359    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:48:58.946955    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0128 16:48:58.946991    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0128 16:48:58.947180    8155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 16:48:59.074524    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:48:59.082009    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:48:59.096761    8155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/joy/.minikube/machines/minikube/id_rsa Username:docker}
I0128 16:48:59.541701    8155 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 16:48:59.601156    8155 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0128 16:48:59.755349    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0128 16:48:59.755386    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0128 16:48:59.972870    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0128 16:48:59.972910    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0128 16:49:00.124953    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0128 16:49:00.124988    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0128 16:49:00.284406    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0128 16:49:00.284437    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0128 16:49:00.417157    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0128 16:49:00.417196    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0128 16:49:00.568226    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0128 16:49:00.568265    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0128 16:49:00.677723    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0128 16:49:00.677755    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0128 16:49:00.801517    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0128 16:49:00.801556    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0128 16:49:00.923972    8155 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0128 16:49:00.924012    8155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0128 16:49:01.042662    8155 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0128 16:49:05.089279    8155 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (6.548831457s)
I0128 16:49:05.089554    8155 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0128 16:49:05.089656    8155 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (6.439231191s)
I0128 16:49:05.089721    8155 api_server.go:52] waiting for apiserver process to appear ...
I0128 16:49:05.089865    8155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 16:49:10.269725    8155 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (10.668467922s)
I0128 16:49:10.269838    8155 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.728087208s)
I0128 16:49:11.323602    8155 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (10.280845518s)
I0128 16:49:11.323717    8155 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (6.23379517s)
I0128 16:49:11.323775    8155 api_server.go:72] duration metric: took 12.753845285s to wait for apiserver process to appear ...
I0128 16:49:11.326842    8155 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0128 16:49:11.323801    8155 api_server.go:88] waiting for apiserver healthz status ...
I0128 16:49:11.329606    8155 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 16:49:11.332947    8155 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0128 16:49:11.337638    8155 addons.go:502] enable addons completed in 12.797043689s: enabled=[storage-provisioner default-storageclass dashboard]
I0128 16:49:11.346441    8155 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0128 16:49:11.349097    8155 api_server.go:141] control plane version: v1.28.3
I0128 16:49:11.349122    8155 api_server.go:131] duration metric: took 19.569856ms to wait for apiserver health ...
I0128 16:49:11.349140    8155 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 16:49:11.366677    8155 system_pods.go:59] 7 kube-system pods found
I0128 16:49:11.366711    8155 system_pods.go:61] "coredns-5dd5756b68-qtflq" [8ffd43dc-6f9b-4b14-93c5-401cc05acf60] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 16:49:11.366723    8155 system_pods.go:61] "etcd-minikube" [874fcf5a-5b38-4d76-8fdb-e7fa251d7bb2] Running
I0128 16:49:11.366736    8155 system_pods.go:61] "kube-apiserver-minikube" [e36fa7fe-1c12-4b7c-a5ee-d6e0e6407ccc] Running
I0128 16:49:11.366746    8155 system_pods.go:61] "kube-controller-manager-minikube" [b855f90a-1e00-47ec-8880-f583dead332e] Running
I0128 16:49:11.366755    8155 system_pods.go:61] "kube-proxy-6w7wj" [37588c65-57ab-4237-9b2f-30ff380ae1ec] Running
I0128 16:49:11.366764    8155 system_pods.go:61] "kube-scheduler-minikube" [3b6d5394-3ec1-4588-8584-d50c0b069bf4] Running
I0128 16:49:11.366773    8155 system_pods.go:61] "storage-provisioner" [a939272e-11d3-4775-bd6f-0e6573e7403a] Running
I0128 16:49:11.366783    8155 system_pods.go:74] duration metric: took 17.632614ms to wait for pod list to return data ...
I0128 16:49:11.366801    8155 kubeadm.go:581] duration metric: took 12.796884636s to wait for : map[apiserver:true system_pods:true] ...
I0128 16:49:11.366824    8155 node_conditions.go:102] verifying NodePressure condition ...
I0128 16:49:11.377140    8155 node_conditions.go:122] node storage ephemeral capacity is 244505916Ki
I0128 16:49:11.377174    8155 node_conditions.go:123] node cpu capacity is 4
I0128 16:49:11.377205    8155 node_conditions.go:105] duration metric: took 10.367704ms to run NodePressure ...
I0128 16:49:11.377244    8155 start.go:228] waiting for startup goroutines ...
I0128 16:49:11.377267    8155 start.go:233] waiting for cluster config update ...
I0128 16:49:11.377296    8155 start.go:242] writing updated cluster config ...
I0128 16:49:11.378340    8155 ssh_runner.go:195] Run: rm -f paused
I0128 16:49:13.070716    8155 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0128 16:49:13.079557    8155 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 28 13:47:59 minikube dockerd[181]: time="2024-01-28T13:47:59.204680153Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jan 28 13:47:59 minikube dockerd[181]: time="2024-01-28T13:47:59.206051693Z" level=info msg="Daemon shutdown complete"
Jan 28 13:47:59 minikube dockerd[181]: time="2024-01-28T13:47:59.206243856Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jan 28 13:47:59 minikube systemd[1]: docker.service: Deactivated successfully.
Jan 28 13:47:59 minikube systemd[1]: Stopped Docker Application Container Engine.
Jan 28 13:47:59 minikube systemd[1]: docker.service: Consumed 2.173s CPU time.
Jan 28 13:47:59 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 28 13:47:59 minikube dockerd[575]: time="2024-01-28T13:47:59.725720270Z" level=info msg="Starting up"
Jan 28 13:47:59 minikube dockerd[575]: time="2024-01-28T13:47:59.761177458Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jan 28 13:47:59 minikube dockerd[575]: time="2024-01-28T13:47:59.925825323Z" level=info msg="Loading containers: start."
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.023773573Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.295614209Z" level=info msg="Loading containers: done."
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.395361479Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.395720329Z" level=info msg="Daemon has completed initialization"
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.426884195Z" level=info msg="Processing signal 'terminated'"
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.495083162Z" level=info msg="API listen on /var/run/docker.sock"
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.495219814Z" level=info msg="API listen on [::]:2376"
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.498685234Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jan 28 13:48:01 minikube dockerd[575]: time="2024-01-28T13:48:01.502920720Z" level=info msg="Daemon shutdown complete"
Jan 28 13:48:01 minikube systemd[1]: docker.service: Deactivated successfully.
Jan 28 13:48:01 minikube systemd[1]: Stopped Docker Application Container Engine.
Jan 28 13:48:01 minikube systemd[1]: docker.service: Consumed 1.515s CPU time.
Jan 28 13:48:01 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 28 13:48:01 minikube dockerd[844]: time="2024-01-28T13:48:01.751217771Z" level=info msg="Starting up"
Jan 28 13:48:01 minikube dockerd[844]: time="2024-01-28T13:48:01.779130801Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jan 28 13:48:01 minikube dockerd[844]: time="2024-01-28T13:48:01.895614749Z" level=info msg="Loading containers: start."
Jan 28 13:48:02 minikube dockerd[844]: time="2024-01-28T13:48:02.788801086Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 28 13:48:02 minikube dockerd[844]: time="2024-01-28T13:48:02.990681541Z" level=info msg="Loading containers: done."
Jan 28 13:48:03 minikube dockerd[844]: time="2024-01-28T13:48:03.091221623Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Jan 28 13:48:03 minikube dockerd[844]: time="2024-01-28T13:48:03.091479449Z" level=info msg="Daemon has completed initialization"
Jan 28 13:48:03 minikube dockerd[844]: time="2024-01-28T13:48:03.202951253Z" level=info msg="API listen on /var/run/docker.sock"
Jan 28 13:48:03 minikube dockerd[844]: time="2024-01-28T13:48:03.202956509Z" level=info msg="API listen on [::]:2376"
Jan 28 13:48:03 minikube systemd[1]: Started Docker Application Container Engine.
Jan 28 13:48:05 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Start docker client with request timeout 0s"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Loaded network plugin cni"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Docker Info: &{ID:77f61143-53c7-40b8-817f-eb03fd8f1252 Containers:19 ContainersRunning:0 ContainersPaused:0 ContainersStopped:19 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:36 SystemTime:2024-01-28T13:48:07.524478653Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:6.5.0-15-generic OperatingSystem:Ubuntu 22.04.3 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0002c4c40 NCPU:4 MemTotal:8049299456 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Setting cgroupDriver systemd"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 28 13:48:07 minikube cri-dockerd[1073]: time="2024-01-28T13:48:07Z" level=info msg="Start cri-dockerd grpc backend"
Jan 28 13:48:07 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 28 13:48:34 minikube cri-dockerd[1073]: time="2024-01-28T13:48:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-pdk2b_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ce7bd7a08490330c832280a6e140927c284a47eb334cc763559c205376de1614\""
Jan 28 13:48:34 minikube cri-dockerd[1073]: time="2024-01-28T13:48:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-hlswt_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"57b992f8dee1f619a184ff8a1adb86ca67e5c0cb3e56d42d16b0cd7272743eb3\""
Jan 28 13:48:34 minikube cri-dockerd[1073]: time="2024-01-28T13:48:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-qtflq_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"01ca9a71d75dbd11733182a8cd65a25e275a6aafe36561aefb74b47df939d6b6\""
Jan 28 13:48:36 minikube cri-dockerd[1073]: time="2024-01-28T13:48:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d64728e292032e389158e357aa71646a31a32d7f68e7f4f5d41ace86ae08c200/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:36 minikube cri-dockerd[1073]: time="2024-01-28T13:48:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e65799ace0b82607652564f166544ead0fb2ee73514c158288ad3cbf5f62c0f/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:37 minikube cri-dockerd[1073]: time="2024-01-28T13:48:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/35693f60d5223b4286af2f23c9f14a629b09e0c3aef48297eccabc4911c6299c/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:37 minikube cri-dockerd[1073]: time="2024-01-28T13:48:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/67f27ebd5115e46cf0f01183c8e6e170cb883acb6dd030740237bb7b767a6fb6/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:53 minikube cri-dockerd[1073]: time="2024-01-28T13:48:53Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 28 13:48:56 minikube cri-dockerd[1073]: time="2024-01-28T13:48:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f6ce4e97d0302a1a95990aa59dabe6d0d3e504b7e9cf9d3064b0a360cbda4585/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:56 minikube cri-dockerd[1073]: time="2024-01-28T13:48:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0dfec6008e4b183e3056736909cc204f8a4407d26af52f471317dc6acffd8dfa/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:57 minikube cri-dockerd[1073]: time="2024-01-28T13:48:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a17b7e002dc60037c4132af51487fcdd47de61e7a8a4b34f24312681f9292d59/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Jan 28 13:48:57 minikube cri-dockerd[1073]: time="2024-01-28T13:48:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/045929d139580f7cdd3fbf4b10951be296722dd68817a7c8e9722c761913ddb5/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local home options ndots:5]"
Jan 28 13:48:57 minikube cri-dockerd[1073]: time="2024-01-28T13:48:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98850f4e4104ae6797704c6ae856be432872a6e57d0c1cefe3b4cd726620291f/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local home options ndots:5]"
Jan 28 13:49:30 minikube dockerd[844]: time="2024-01-28T13:49:30.615095500Z" level=info msg="ignoring event" container=9137ed4b79c3544bd711a83a2433890d036298b1e9ddbabdc4cf2983130a4f2f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 13:49:35 minikube dockerd[844]: time="2024-01-28T13:49:35.477086265Z" level=info msg="ignoring event" container=a33fb1c4f6164387fbb5bf40cdea493cb25bb8fe6809f44291fe7d21a04b10e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
5e938fde846bf       07655ddf2eebe                                                                                          31 minutes ago      Running             kubernetes-dashboard        2                   045929d139580       kubernetes-dashboard-8694d4445c-pdk2b
c02f0c52dff10       6e38f40d628db                                                                                          31 minutes ago      Running             storage-provisioner         3                   0dfec6008e4b1       storage-provisioner
a33fb1c4f6164       07655ddf2eebe                                                                                          32 minutes ago      Exited              kubernetes-dashboard        1                   045929d139580       kubernetes-dashboard-8694d4445c-pdk2b
f94ac14b28850       ead0a4a53df89                                                                                          32 minutes ago      Running             coredns                     1                   a17b7e002dc60       coredns-5dd5756b68-qtflq
908a2003a756e       115053965e86b                                                                                          32 minutes ago      Running             dashboard-metrics-scraper   1                   98850f4e4104a       dashboard-metrics-scraper-7fd5cb4ddc-hlswt
fbbe09bad200c       bfc896cf80fba                                                                                          32 minutes ago      Running             kube-proxy                  1                   f6ce4e97d0302       kube-proxy-6w7wj
9137ed4b79c35       6e38f40d628db                                                                                          32 minutes ago      Exited              storage-provisioner         2                   0dfec6008e4b1       storage-provisioner
b57fdde03e02e       5374347291230                                                                                          32 minutes ago      Running             kube-apiserver              1                   7e65799ace0b8       kube-apiserver-minikube
7effe5b7db861       6d1b4fd1b182d                                                                                          32 minutes ago      Running             kube-scheduler              1                   67f27ebd5115e       kube-scheduler-minikube
7a3b783855c62       73deb9a3f7025                                                                                          32 minutes ago      Running             etcd                        1                   35693f60d5223       etcd-minikube
a3c291655c2d9       10baa1ca17068                                                                                          32 minutes ago      Running             kube-controller-manager     1                   d64728e292032       kube-controller-manager-minikube
ac46838b239c6       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   3 days ago          Exited              dashboard-metrics-scraper   0                   57b992f8dee1f       dashboard-metrics-scraper-7fd5cb4ddc-hlswt
342d748b54307       ead0a4a53df89                                                                                          3 days ago          Exited              coredns                     0                   01ca9a71d75db       coredns-5dd5756b68-qtflq
86a19c6c15998       bfc896cf80fba                                                                                          3 days ago          Exited              kube-proxy                  0                   7a0c68f4c7002       kube-proxy-6w7wj
159ca09057972       10baa1ca17068                                                                                          3 days ago          Exited              kube-controller-manager     0                   8fc077ebc747b       kube-controller-manager-minikube
d99a4c831f550       73deb9a3f7025                                                                                          3 days ago          Exited              etcd                        0                   486fa507b4011       etcd-minikube
9045a5d0b3886       6d1b4fd1b182d                                                                                          3 days ago          Exited              kube-scheduler              0                   f601c5a6c1138       kube-scheduler-minikube
74d4edc1bc7db       5374347291230                                                                                          3 days ago          Exited              kube-apiserver              0                   18ef25765176d       kube-apiserver-minikube

* 
* ==> coredns [342d748b5430] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:53646 - 32018 "HINFO IN 8682011923449233101.1949482636603819792. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.218142139s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> coredns [f94ac14b2885] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:48503 - 55914 "HINFO IN 8645357662550836035.4353748412071170518. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.355844151s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_25T17_19_08_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 25 Jan 2024 14:19:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Jan 2024 14:21:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Jan 2024 14:19:41 +0000   Thu, 25 Jan 2024 14:19:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Jan 2024 14:19:41 +0000   Thu, 25 Jan 2024 14:19:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Jan 2024 14:19:41 +0000   Thu, 25 Jan 2024 14:19:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Jan 2024 14:19:41 +0000   Thu, 25 Jan 2024 14:19:04 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  244505916Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7860644Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  244505916Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7860644Ki
  pods:               110
System Info:
  Machine ID:                 2a9aee9d9541494a93546a750798b4c7
  System UUID:                055ffb8c-3587-4730-a875-3050e45613f3
  Boot ID:                    655d3557-9a20-4e77-aff2-f2cdc6a15270
  Kernel Version:             6.5.0-15-generic
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5dd5756b68-qtflq                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     3d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3d
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kube-system                 kube-proxy-6w7wj                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-hlswt    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d23h
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-pdk2b         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 32m                kube-proxy       
  Normal  Starting                 32m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     32m (x7 over 32m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  32m (x8 over 32m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    32m (x8 over 32m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  RegisteredNode           32m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000081/00006000
[  +0.000012] ath10k_pci 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.000009] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.305889] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Physical Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000001/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.041959] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.002394] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000009] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.000259] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000007] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000008] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.013109] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000011] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000012] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.018680] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.098595] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000012] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000013] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.413272] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.080078] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.016492] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000013] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000015] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.010848] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Physical Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000001/00006000
[  +0.000011] ath10k_pci 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.024409] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.239566] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000011] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.011971] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.078859] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000009] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.310597] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000014] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000016] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.001563] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000013] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000014] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.009834] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000010] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000010] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               
[  +0.052429] ath10k_pci 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Receiver ID)
[  +0.000013] ath10k_pci 0000:02:00.0:   device [168c:003e] error status/mask=00000080/00006000
[  +0.000014] ath10k_pci 0000:02:00.0:    [ 7] BadDLLP               

* 
* ==> etcd [7a3b783855c6] <==
* {"level":"info","ts":"2024-01-28T14:19:36.65838Z","caller":"traceutil/trace.go:171","msg":"trace[1152070579] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:13693; }","duration":"169.186869ms","start":"2024-01-28T14:19:36.489095Z","end":"2024-01-28T14:19:36.658282Z","steps":["trace[1152070579] 'agreement among raft nodes before linearized reading'  (duration: 167.769866ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:19:37.260091Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"266.065245ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649116955 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13686 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026803649116951 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:19:37.260289Z","caller":"traceutil/trace.go:171","msg":"trace[1210777000] transaction","detail":"{read_only:false; response_revision:13694; number_of_response:1; }","duration":"601.411068ms","start":"2024-01-28T14:19:36.658836Z","end":"2024-01-28T14:19:37.260247Z","steps":["trace[1210777000] 'process raft request'  (duration: 335.027609ms)","trace[1210777000] 'compare'  (duration: 265.071266ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:19:37.260495Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:19:36.658791Z","time spent":"601.59564ms","remote":"127.0.0.1:45434","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13686 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026803649116951 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-01-28T14:20:03.914427Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"246.292316ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649117057 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:13708 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:20:03.914624Z","caller":"traceutil/trace.go:171","msg":"trace[1549705017] linearizableReadLoop","detail":"{readStateIndex:17097; appliedIndex:17096; }","duration":"281.716793ms","start":"2024-01-28T14:20:03.63287Z","end":"2024-01-28T14:20:03.914587Z","steps":["trace[1549705017] 'read index received'  (duration: 34.627835ms)","trace[1549705017] 'applied index is now lower than readState.Index'  (duration: 247.086202ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:03.914859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"282.025756ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-28T14:20:03.914957Z","caller":"traceutil/trace.go:171","msg":"trace[1795411110] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:13716; }","duration":"282.131722ms","start":"2024-01-28T14:20:03.632798Z","end":"2024-01-28T14:20:03.91493Z","steps":["trace[1795411110] 'agreement among raft nodes before linearized reading'  (duration: 281.914124ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:03.915458Z","caller":"traceutil/trace.go:171","msg":"trace[256189454] transaction","detail":"{read_only:false; response_revision:13716; number_of_response:1; }","duration":"442.003925ms","start":"2024-01-28T14:20:03.47342Z","end":"2024-01-28T14:20:03.915424Z","steps":["trace[256189454] 'process raft request'  (duration: 194.64066ms)","trace[256189454] 'compare'  (duration: 245.395612ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:03.915686Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:03.47335Z","time spent":"442.209887ms","remote":"127.0.0.1:45680","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:13708 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-01-28T14:20:06.677096Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"389.006491ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2024-01-28T14:20:06.67722Z","caller":"traceutil/trace.go:171","msg":"trace[953170087] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:13717; }","duration":"389.147237ms","start":"2024-01-28T14:20:06.288041Z","end":"2024-01-28T14:20:06.677188Z","steps":["trace[953170087] 'range keys from bolt db'  (duration: 388.754109ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:06.677333Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:06.288011Z","time spent":"389.289778ms","remote":"127.0.0.1:45434","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":157,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-01-28T14:20:07.654974Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"736.142999ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649117067 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13710 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026803649117065 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:20:07.655711Z","caller":"traceutil/trace.go:171","msg":"trace[1271016637] linearizableReadLoop","detail":"{readStateIndex:17100; appliedIndex:17099; }","duration":"624.890404ms","start":"2024-01-28T14:20:07.030787Z","end":"2024-01-28T14:20:07.655677Z","steps":["trace[1271016637] 'read index received'  (duration: 88.697¬µs)","trace[1271016637] 'applied index is now lower than readState.Index'  (duration: 624.79764ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:07.657996Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"627.23769ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-28T14:20:07.659929Z","caller":"traceutil/trace.go:171","msg":"trace[1364096263] range","detail":"{range_begin:/registry/endpointslices/; range_end:/registry/endpointslices0; response_count:0; response_revision:13718; }","duration":"628.958279ms","start":"2024-01-28T14:20:07.030708Z","end":"2024-01-28T14:20:07.659666Z","steps":["trace[1364096263] 'agreement among raft nodes before linearized reading'  (duration: 626.959884ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:07.657145Z","caller":"traceutil/trace.go:171","msg":"trace[950580125] transaction","detail":"{read_only:false; response_revision:13718; number_of_response:1; }","duration":"868.021581ms","start":"2024-01-28T14:20:06.789077Z","end":"2024-01-28T14:20:07.657099Z","steps":["trace[950580125] 'process raft request'  (duration: 129.635141ms)","trace[950580125] 'compare'  (duration: 735.640134ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:07.658243Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"426.8469ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2024-01-28T14:20:07.658371Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"256.148932ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-01-28T14:20:07.660467Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:07.030677Z","time spent":"629.745794ms","remote":"127.0.0.1:45684","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":4,"response size":31,"request content":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true "}
{"level":"info","ts":"2024-01-28T14:20:07.660912Z","caller":"traceutil/trace.go:171","msg":"trace[249779095] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:13718; }","duration":"429.577301ms","start":"2024-01-28T14:20:07.231295Z","end":"2024-01-28T14:20:07.660873Z","steps":["trace[249779095] 'agreement among raft nodes before linearized reading'  (duration: 426.804145ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:07.661334Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:07.231266Z","time spent":"430.042228ms","remote":"127.0.0.1:45606","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-01-28T14:20:07.660814Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:06.789042Z","time spent":"871.672968ms","remote":"127.0.0.1:45434","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13710 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026803649117065 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-01-28T14:20:07.661065Z","caller":"traceutil/trace.go:171","msg":"trace[1691966519] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13718; }","duration":"258.840383ms","start":"2024-01-28T14:20:07.402205Z","end":"2024-01-28T14:20:07.661045Z","steps":["trace[1691966519] 'agreement among raft nodes before linearized reading'  (duration: 256.114748ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:08.065102Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.965109ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2024-01-28T14:20:08.065357Z","caller":"traceutil/trace.go:171","msg":"trace[1310322158] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:13718; }","duration":"398.396048ms","start":"2024-01-28T14:20:07.666884Z","end":"2024-01-28T14:20:08.06528Z","steps":["trace[1310322158] 'range keys from bolt db'  (duration: 397.740686ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:08.065558Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:07.666857Z","time spent":"398.647817ms","remote":"127.0.0.1:45606","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":444,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"warn","ts":"2024-01-28T14:20:08.296506Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"154.481935ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649117071 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13717 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:20:08.296674Z","caller":"traceutil/trace.go:171","msg":"trace[475635840] linearizableReadLoop","detail":"{readStateIndex:17101; appliedIndex:17100; }","duration":"428.880707ms","start":"2024-01-28T14:20:07.867759Z","end":"2024-01-28T14:20:08.29664Z","steps":["trace[475635840] 'read index received'  (duration: 274.236527ms)","trace[475635840] 'applied index is now lower than readState.Index'  (duration: 154.640801ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T14:20:08.296851Z","caller":"traceutil/trace.go:171","msg":"trace[1707993248] transaction","detail":"{read_only:false; response_revision:13719; number_of_response:1; }","duration":"619.787653ms","start":"2024-01-28T14:20:07.677022Z","end":"2024-01-28T14:20:08.29681Z","steps":["trace[1707993248] 'process raft request'  (duration: 464.874504ms)","trace[1707993248] 'compare'  (duration: 154.293791ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:08.297072Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:07.676984Z","time spent":"619.963538ms","remote":"127.0.0.1:45606","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13717 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-01-28T14:20:08.296894Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"429.180296ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T14:20:08.297465Z","caller":"traceutil/trace.go:171","msg":"trace[13568252] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:13719; }","duration":"429.749968ms","start":"2024-01-28T14:20:07.867681Z","end":"2024-01-28T14:20:08.297431Z","steps":["trace[13568252] 'agreement among raft nodes before linearized reading'  (duration: 429.131018ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:08.297594Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:07.867649Z","time spent":"429.903488ms","remote":"127.0.0.1:45718","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":0,"response size":29,"request content":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true "}
{"level":"warn","ts":"2024-01-28T14:20:08.298226Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"229.141147ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:133"}
{"level":"warn","ts":"2024-01-28T14:20:08.298264Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.363809ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T14:20:08.298331Z","caller":"traceutil/trace.go:171","msg":"trace[896493240] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:13719; }","duration":"229.350614ms","start":"2024-01-28T14:20:08.068952Z","end":"2024-01-28T14:20:08.298302Z","steps":["trace[896493240] 'agreement among raft nodes before linearized reading'  (duration: 229.060281ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:08.298444Z","caller":"traceutil/trace.go:171","msg":"trace[251860555] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:13719; }","duration":"101.48031ms","start":"2024-01-28T14:20:08.19686Z","end":"2024-01-28T14:20:08.29834Z","steps":["trace[251860555] 'agreement among raft nodes before linearized reading'  (duration: 101.249469ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:08.298803Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.56521ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T14:20:08.298891Z","caller":"traceutil/trace.go:171","msg":"trace[1853852940] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:13719; }","duration":"193.661415ms","start":"2024-01-28T14:20:08.105203Z","end":"2024-01-28T14:20:08.298865Z","steps":["trace[1853852940] 'agreement among raft nodes before linearized reading'  (duration: 193.515091ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:12.788093Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"152.649003ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-28T14:20:12.789088Z","caller":"traceutil/trace.go:171","msg":"trace[488011793] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:13722; }","duration":"152.851265ms","start":"2024-01-28T14:20:12.635381Z","end":"2024-01-28T14:20:12.788232Z","steps":["trace[488011793] 'count revisions from in-memory index tree'  (duration: 152.439799ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:18.718621Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.810371ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649117110 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13726 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:20:18.719011Z","caller":"traceutil/trace.go:171","msg":"trace[1755615845] transaction","detail":"{read_only:false; response_revision:13727; number_of_response:1; }","duration":"232.521886ms","start":"2024-01-28T14:20:18.486421Z","end":"2024-01-28T14:20:18.718942Z","steps":["trace[1755615845] 'process raft request'  (duration: 61.145605ms)","trace[1755615845] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 170.516734ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T14:20:20.499755Z","caller":"traceutil/trace.go:171","msg":"trace[409278619] transaction","detail":"{read_only:false; response_revision:13728; number_of_response:1; }","duration":"102.610018ms","start":"2024-01-28T14:20:20.39709Z","end":"2024-01-28T14:20:20.4997Z","steps":["trace[409278619] 'process raft request'  (duration: 89.343144ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:26.464568Z","caller":"traceutil/trace.go:171","msg":"trace[1032205107] transaction","detail":"{read_only:false; response_revision:13733; number_of_response:1; }","duration":"110.698879ms","start":"2024-01-28T14:20:26.353816Z","end":"2024-01-28T14:20:26.464515Z","steps":["trace[1032205107] 'process raft request'  (duration: 43.438953ms)","trace[1032205107] 'compare'  (duration: 66.323096ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T14:20:29.089119Z","caller":"traceutil/trace.go:171","msg":"trace[1195326939] transaction","detail":"{read_only:false; response_revision:13735; number_of_response:1; }","duration":"142.240758ms","start":"2024-01-28T14:20:28.94683Z","end":"2024-01-28T14:20:29.089071Z","steps":["trace[1195326939] 'process raft request'  (duration: 141.969647ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:29.462487Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"294.876112ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-28T14:20:29.463336Z","caller":"traceutil/trace.go:171","msg":"trace[777786959] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:13735; }","duration":"295.732337ms","start":"2024-01-28T14:20:29.167562Z","end":"2024-01-28T14:20:29.463295Z","steps":["trace[777786959] 'count revisions from in-memory index tree'  (duration: 294.643189ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:31.201235Z","caller":"traceutil/trace.go:171","msg":"trace[752633910] transaction","detail":"{read_only:false; response_revision:13736; number_of_response:1; }","duration":"118.766555ms","start":"2024-01-28T14:20:31.0824Z","end":"2024-01-28T14:20:31.201166Z","steps":["trace[752633910] 'process raft request'  (duration: 112.550923ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:31.607095Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.034824ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026803649117153 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13735 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T14:20:31.607294Z","caller":"traceutil/trace.go:171","msg":"trace[493779179] linearizableReadLoop","detail":"{readStateIndex:17123; appliedIndex:17122; }","duration":"209.201514ms","start":"2024-01-28T14:20:31.398056Z","end":"2024-01-28T14:20:31.607258Z","steps":["trace[493779179] 'read index received'  (duration: 46.725958ms)","trace[493779179] 'applied index is now lower than readState.Index'  (duration: 162.472309ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:31.60752Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"209.510145ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T14:20:31.607603Z","caller":"traceutil/trace.go:171","msg":"trace[233457849] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13737; }","duration":"209.596406ms","start":"2024-01-28T14:20:31.397982Z","end":"2024-01-28T14:20:31.607578Z","steps":["trace[233457849] 'agreement among raft nodes before linearized reading'  (duration: 209.384388ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T14:20:31.608185Z","caller":"traceutil/trace.go:171","msg":"trace[337701726] transaction","detail":"{read_only:false; response_revision:13737; number_of_response:1; }","duration":"388.105967ms","start":"2024-01-28T14:20:31.220029Z","end":"2024-01-28T14:20:31.608135Z","steps":["trace[337701726] 'process raft request'  (duration: 224.879797ms)","trace[337701726] 'compare'  (duration: 161.839282ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T14:20:31.608443Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:31.219991Z","time spent":"388.292523ms","remote":"127.0.0.1:45606","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13735 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-01-28T14:20:32.666707Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"671.084042ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:887"}
{"level":"info","ts":"2024-01-28T14:20:32.666804Z","caller":"traceutil/trace.go:171","msg":"trace[602177603] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:13737; }","duration":"671.198443ms","start":"2024-01-28T14:20:31.99558Z","end":"2024-01-28T14:20:32.666778Z","steps":["trace[602177603] 'range keys from bolt db'  (duration: 670.839698ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T14:20:32.666886Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T14:20:31.995549Z","time spent":"671.310814ms","remote":"127.0.0.1:45606","response type":"/etcdserverpb.KV/Range","request count":0,"request size":77,"response count":1,"response size":911,"request content":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" "}

* 
* ==> etcd [d99a4c831f55] <==
* {"level":"info","ts":"2024-01-25T17:33:24.535319Z","caller":"traceutil/trace.go:171","msg":"trace[1609857211] transaction","detail":"{read_only:false; response_revision:9761; number_of_response:1; }","duration":"111.274121ms","start":"2024-01-25T17:33:24.423999Z","end":"2024-01-25T17:33:24.535273Z","steps":["trace[1609857211] 'process raft request'  (duration: 110.670168ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T17:33:24.876197Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.916043ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-25T17:33:24.876375Z","caller":"traceutil/trace.go:171","msg":"trace[1625541646] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9761; }","duration":"124.108212ms","start":"2024-01-25T17:33:24.752229Z","end":"2024-01-25T17:33:24.876317Z","steps":["trace[1625541646] 'range keys from in-memory index tree'  (duration: 123.713365ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:26.723677Z","caller":"traceutil/trace.go:171","msg":"trace[1419192167] transaction","detail":"{read_only:false; response_revision:9762; number_of_response:1; }","duration":"172.248787ms","start":"2024-01-25T17:33:26.551381Z","end":"2024-01-25T17:33:26.72363Z","steps":["trace[1419192167] 'process raft request'  (duration: 171.972099ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:27.673242Z","caller":"traceutil/trace.go:171","msg":"trace[1800669015] transaction","detail":"{read_only:false; response_revision:9763; number_of_response:1; }","duration":"161.213333ms","start":"2024-01-25T17:33:27.511956Z","end":"2024-01-25T17:33:27.673169Z","steps":["trace[1800669015] 'process raft request'  (duration: 79.762014ms)","trace[1800669015] 'compare'  (duration: 66.250797ms)","trace[1800669015] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 15.029767ms)"],"step_count":3}
{"level":"warn","ts":"2024-01-25T17:33:28.919498Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.723926ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-01-25T17:33:28.919748Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"187.900263ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-01-25T17:33:28.919917Z","caller":"traceutil/trace.go:171","msg":"trace[1824696929] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:9764; }","duration":"188.067595ms","start":"2024-01-25T17:33:28.731801Z","end":"2024-01-25T17:33:28.919868Z","steps":["trace[1824696929] 'range keys from in-memory index tree'  (duration: 187.566328ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:28.919683Z","caller":"traceutil/trace.go:171","msg":"trace[1171051003] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9764; }","duration":"166.921464ms","start":"2024-01-25T17:33:28.752715Z","end":"2024-01-25T17:33:28.919637Z","steps":["trace[1171051003] 'range keys from in-memory index tree'  (duration: 166.485759ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T17:33:31.153304Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.242494ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-proxy-6w7wj\" ","response":"range_response_count:1 size:4419"}
{"level":"info","ts":"2024-01-25T17:33:31.153442Z","caller":"traceutil/trace.go:171","msg":"trace[602510291] range","detail":"{range_begin:/registry/pods/kube-system/kube-proxy-6w7wj; range_end:; response_count:1; response_revision:9766; }","duration":"136.387881ms","start":"2024-01-25T17:33:31.017014Z","end":"2024-01-25T17:33:31.153402Z","steps":["trace[602510291] 'agreement among raft nodes before linearized reading'  (duration: 51.836769ms)","trace[602510291] 'range keys from bolt db'  (duration: 84.321568ms)"],"step_count":2}
{"level":"info","ts":"2024-01-25T17:33:31.933984Z","caller":"traceutil/trace.go:171","msg":"trace[522143811] transaction","detail":"{read_only:false; response_revision:9767; number_of_response:1; }","duration":"103.923674ms","start":"2024-01-25T17:33:31.830013Z","end":"2024-01-25T17:33:31.933936Z","steps":["trace[522143811] 'process raft request'  (duration: 103.627132ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T17:33:33.579954Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.444348ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-25T17:33:33.580115Z","caller":"traceutil/trace.go:171","msg":"trace[703910876] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:9768; }","duration":"284.624286ms","start":"2024-01-25T17:33:33.295453Z","end":"2024-01-25T17:33:33.580077Z","steps":["trace[703910876] 'count revisions from in-memory index tree'  (duration: 284.107738ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:37.363891Z","caller":"traceutil/trace.go:171","msg":"trace[818159299] transaction","detail":"{read_only:false; response_revision:9770; number_of_response:1; }","duration":"110.130169ms","start":"2024-01-25T17:33:37.25343Z","end":"2024-01-25T17:33:37.36356Z","steps":["trace[818159299] 'process raft request'  (duration: 109.346074ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:37.592363Z","caller":"traceutil/trace.go:171","msg":"trace[46469789] transaction","detail":"{read_only:false; response_revision:9771; number_of_response:1; }","duration":"183.19941ms","start":"2024-01-25T17:33:37.409114Z","end":"2024-01-25T17:33:37.592313Z","steps":["trace[46469789] 'process raft request'  (duration: 100.256295ms)","trace[46469789] 'compare'  (duration: 82.234953ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T17:33:41.953264Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.846881ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-25T17:33:41.953743Z","caller":"traceutil/trace.go:171","msg":"trace[944328369] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9774; }","duration":"201.394727ms","start":"2024-01-25T17:33:41.752307Z","end":"2024-01-25T17:33:41.953702Z","steps":["trace[944328369] 'range keys from in-memory index tree'  (duration: 200.455015ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:33:42.490096Z","caller":"traceutil/trace.go:171","msg":"trace[1350192554] transaction","detail":"{read_only:false; response_revision:9775; number_of_response:1; }","duration":"148.627107ms","start":"2024-01-25T17:33:42.341436Z","end":"2024-01-25T17:33:42.490063Z","steps":["trace[1350192554] 'process raft request'  (duration: 148.434825ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T17:33:48.276928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"142.19919ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:887"}
{"level":"info","ts":"2024-01-25T17:33:48.277084Z","caller":"traceutil/trace.go:171","msg":"trace[328483748] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:9779; }","duration":"142.36354ms","start":"2024-01-25T17:33:48.134676Z","end":"2024-01-25T17:33:48.27704Z","steps":["trace[328483748] 'range keys from bolt db'  (duration: 141.830472ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:34:04.61123Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9562}
{"level":"info","ts":"2024-01-25T17:34:04.716558Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9562,"took":"104.415005ms","hash":1560502884}
{"level":"info","ts":"2024-01-25T17:34:04.716636Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1560502884,"revision":9562,"compact-revision":9323}
{"level":"info","ts":"2024-01-25T17:36:09.357077Z","caller":"traceutil/trace.go:171","msg":"trace[1771298186] linearizableReadLoop","detail":"{readStateIndex:12306; appliedIndex:12305; }","duration":"195.345761ms","start":"2024-01-25T17:36:09.161689Z","end":"2024-01-25T17:36:09.357034Z","steps":["trace[1771298186] 'read index received'  (duration: 195.101467ms)","trace[1771298186] 'applied index is now lower than readState.Index'  (duration: 241.519¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T17:36:09.35734Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.674065ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-25T17:36:09.357365Z","caller":"traceutil/trace.go:171","msg":"trace[1513606867] transaction","detail":"{read_only:false; response_revision:9893; number_of_response:1; }","duration":"319.759423ms","start":"2024-01-25T17:36:09.037566Z","end":"2024-01-25T17:36:09.357325Z","steps":["trace[1513606867] 'process raft request'  (duration: 319.193092ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T17:36:09.357458Z","caller":"traceutil/trace.go:171","msg":"trace[314081452] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:9893; }","duration":"195.82172ms","start":"2024-01-25T17:36:09.161608Z","end":"2024-01-25T17:36:09.357429Z","steps":["trace[314081452] 'agreement among raft nodes before linearized reading'  (duration: 195.608156ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T17:36:09.357618Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-25T17:36:09.037518Z","time spent":"319.954517ms","remote":"127.0.0.1:40370","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:9891 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-01-25T17:39:04.620745Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9793}
{"level":"info","ts":"2024-01-25T17:39:04.637421Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9793,"took":"16.444465ms","hash":2567336192}
{"level":"info","ts":"2024-01-25T17:39:04.637464Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2567336192,"revision":9793,"compact-revision":9562}
{"level":"info","ts":"2024-01-25T17:44:04.632092Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10031}
{"level":"info","ts":"2024-01-25T17:44:04.635106Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10031,"took":"2.427717ms","hash":4076229615}
{"level":"info","ts":"2024-01-25T17:44:04.635181Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4076229615,"revision":10031,"compact-revision":9793}
{"level":"info","ts":"2024-01-25T18:10:30.619867Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10271}
{"level":"info","ts":"2024-01-25T18:10:30.624628Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10271,"took":"3.383149ms","hash":1208696752}
{"level":"info","ts":"2024-01-25T18:10:30.625315Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1208696752,"revision":10271,"compact-revision":10031}
{"level":"info","ts":"2024-01-25T18:15:30.634834Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10510}
{"level":"info","ts":"2024-01-25T18:15:30.635879Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10510,"took":"802.901¬µs","hash":2523554979}
{"level":"info","ts":"2024-01-25T18:15:30.635914Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2523554979,"revision":10510,"compact-revision":10271}
{"level":"info","ts":"2024-01-25T18:20:30.644293Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10749}
{"level":"info","ts":"2024-01-25T18:20:30.647759Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10749,"took":"2.338897ms","hash":4142743403}
{"level":"info","ts":"2024-01-25T18:20:30.647831Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4142743403,"revision":10749,"compact-revision":10510}
{"level":"info","ts":"2024-01-25T18:25:30.656739Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10989}
{"level":"info","ts":"2024-01-25T18:25:30.660524Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10989,"took":"3.049045ms","hash":3284295747}
{"level":"info","ts":"2024-01-25T18:25:30.660648Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3284295747,"revision":10989,"compact-revision":10749}
{"level":"info","ts":"2024-01-25T18:27:30.710642Z","caller":"traceutil/trace.go:171","msg":"trace[1582711520] transaction","detail":"{read_only:false; response_revision:11324; number_of_response:1; }","duration":"415.050629ms","start":"2024-01-25T18:27:30.295475Z","end":"2024-01-25T18:27:30.710525Z","steps":["trace[1582711520] 'process raft request'  (duration: 414.739219ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T18:27:30.71136Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-25T18:27:30.295436Z","time spent":"415.363232ms","remote":"127.0.0.1:40370","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11323 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-01-25T18:27:30.784793Z","caller":"traceutil/trace.go:171","msg":"trace[1681134399] transaction","detail":"{read_only:false; response_revision:11325; number_of_response:1; }","duration":"306.534453ms","start":"2024-01-25T18:27:30.478206Z","end":"2024-01-25T18:27:30.78474Z","steps":["trace[1681134399] 'process raft request'  (duration: 294.568493ms)","trace[1681134399] 'compare'  (duration: 11.265804ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T18:27:30.785054Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-25T18:27:30.47819Z","time spent":"306.738946ms","remote":"127.0.0.1:40446","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:11316 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-01-25T18:30:30.668119Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11228}
{"level":"info","ts":"2024-01-25T18:30:30.669896Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11228,"took":"1.302464ms","hash":4009079174}
{"level":"info","ts":"2024-01-25T18:30:30.669948Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4009079174,"revision":11228,"compact-revision":10989}
{"level":"info","ts":"2024-01-25T18:35:30.682649Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11468}
{"level":"info","ts":"2024-01-25T18:35:30.76731Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11468,"took":"83.249982ms","hash":1860563735}
{"level":"info","ts":"2024-01-25T18:35:30.767421Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1860563735,"revision":11468,"compact-revision":11228}
{"level":"info","ts":"2024-01-25T18:40:30.694119Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11707}
{"level":"info","ts":"2024-01-25T18:40:30.698807Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11707,"took":"4.431105ms","hash":4030824198}
{"level":"info","ts":"2024-01-25T18:40:30.698856Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4030824198,"revision":11707,"compact-revision":11468}

* 
* ==> kernel <==
*  14:21:16 up  1:34,  0 users,  load average: 4.34, 4.82, 3.83
Linux minikube 6.5.0-15-generic #15~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [74d4edc1bc7d] <==
* E0125 18:34:22.376968       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:34:32.377460       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:34:42.378022       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:34:52.379188       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:02.380352       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:12.381654       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","catch-all","exempt","system","node-high","leader-election","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:22.382968       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:32.383709       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","system","node-high","leader-election","workload-high","workload-low","global-default","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:42.384626       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:35:52.385483       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","catch-all","exempt","system","node-high","leader-election","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:02.386675       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:12.387529       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:22.389178       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:32.389884       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:42.391188       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","catch-all","exempt","system","node-high","leader-election","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0125 18:36:52.392682       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:02.393641       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","system","node-high","leader-election","workload-high","workload-low","global-default"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:12.394909       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:22.395362       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","system","node-high","leader-election","workload-high","workload-low","global-default","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:32.396061       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:42.396855       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","system","node-high","leader-election","workload-high","workload-low","global-default","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0125 18:37:52.398157       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:02.399197       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:12.399867       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:22.400508       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:32.401050       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:42.401962       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:38:52.402887       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","system","node-high","leader-election","workload-high","workload-low","global-default"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:02.403834       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:12.404712       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:22.406611       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:32.408023       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:42.409268       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:39:52.410975       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["global-default","catch-all","exempt","system","node-high","leader-election","workload-high","workload-low"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:02.412256       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:12.413367       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:22.414729       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:32.415476       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:42.417025       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","system","node-high","leader-election","workload-high","workload-low","global-default"] items=[{},{},{},{},{},{},{},{}]
E0125 18:40:52.418359       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:02.419352       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:12.420878       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:22.422475       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:32.423352       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:42.423968       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:41:52.425830       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:02.427042       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:12.427693       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:22.428995       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-low","global-default","catch-all","exempt","system","node-high","leader-election","workload-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:32.430009       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:42.430830       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:42:52.432019       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["system","node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:02.433347       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","system","node-high","leader-election","workload-high","workload-low","global-default"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:12.434128       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["exempt","system","node-high","leader-election","workload-high","workload-low","global-default","catch-all"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:22.438086       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:32.439324       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["workload-high","workload-low","global-default","catch-all","exempt","system","node-high","leader-election"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:42.440578       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["catch-all","exempt","system","node-high","leader-election","workload-high","workload-low","global-default"] items=[{},{},{},{},{},{},{},{}]
E0125 18:43:52.441359       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]
E0125 18:44:02.442239       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["leader-election","workload-high","workload-low","global-default","catch-all","exempt","system","node-high"] items=[{},{},{},{},{},{},{},{}]
E0125 18:44:12.442910       1 apf_controller.go:419] "Unable to derive new concurrency limits" err="impossible: ran out of bounds to consider in bound-constrained problem" plNames=["node-high","leader-election","workload-high","workload-low","global-default","catch-all","exempt","system"] items=[{},{},{},{},{},{},{},{}]

* 
* ==> kube-apiserver [b57fdde03e02] <==
* I0128 13:49:08.909542       1 controller.go:624] quota admission added evaluator for: endpoints
I0128 13:55:22.900977       1 trace.go:236] Trace[464369542]: "Update" accept:application/json, */*,audit-id:39eecf29-059e-4d6c-bf87-12fd44e4f246,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 13:55:22.193) (total time: 600ms):
Trace[464369542]: ["GuaranteedUpdate etcd3" audit-id:39eecf29-059e-4d6c-bf87-12fd44e4f246,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 624ms (13:55:22.197)
Trace[464369542]:  ---"Txn call completed" 593ms (13:55:22.794)]
Trace[464369542]: [600.838548ms] [600.838548ms] END
I0128 13:55:29.723762       1 trace.go:236] Trace[1433456292]: "Update" accept:application/json, */*,audit-id:01b0f79f-b0d7-46ea-84ae-a5f8ecc2260a,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 13:55:29.212) (total time: 511ms):
Trace[1433456292]: ["GuaranteedUpdate etcd3" audit-id:01b0f79f-b0d7-46ea-84ae-a5f8ecc2260a,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 510ms (13:55:29.212)
Trace[1433456292]:  ---"Txn call completed" 493ms (13:55:29.723)]
Trace[1433456292]: [511.568926ms] [511.568926ms] END
I0128 13:55:36.741793       1 trace.go:236] Trace[2134039773]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 13:55:36.197) (total time: 544ms):
Trace[2134039773]: ---"Transaction prepared" 383ms (13:55:36.585)
Trace[2134039773]: ---"Txn call completed" 156ms (13:55:36.741)
Trace[2134039773]: [544.439517ms] [544.439517ms] END
I0128 14:17:07.691272       1 trace.go:236] Trace[2127035999]: "Get" accept:application/json, */*,audit-id:588399e1-d352-4ee9-b3a2-90bc250fdee0,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (28-Jan-2024 14:17:06.995) (total time: 695ms):
Trace[2127035999]: ---"About to write a response" 695ms (14:17:07.690)
Trace[2127035999]: [695.741119ms] [695.741119ms] END
I0128 14:17:07.692523       1 trace.go:236] Trace[569881891]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 14:17:06.276) (total time: 1416ms):
Trace[569881891]: ---"Transaction prepared" 341ms (14:17:06.619)
Trace[569881891]: ---"Txn call completed" 1072ms (14:17:07.692)
Trace[569881891]: [1.416054089s] [1.416054089s] END
I0128 14:18:26.862014       1 trace.go:236] Trace[332795033]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 14:18:26.280) (total time: 581ms):
Trace[332795033]: ---"Txn call completed" 540ms (14:18:26.861)
Trace[332795033]: [581.148689ms] [581.148689ms] END
I0128 14:18:27.271337       1 trace.go:236] Trace[18145363]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d9287ffe-d622-421f-8720-97e4edbf8f46,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (28-Jan-2024 14:18:26.597) (total time: 673ms):
Trace[18145363]: ["GuaranteedUpdate etcd3" audit-id:d9287ffe-d622-421f-8720-97e4edbf8f46,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 673ms (14:18:26.597)
Trace[18145363]:  ---"Txn call completed" 670ms (14:18:27.270)]
Trace[18145363]: [673.907912ms] [673.907912ms] END
I0128 14:18:47.241716       1 trace.go:236] Trace[1223374393]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (28-Jan-2024 14:18:46.733) (total time: 508ms):
Trace[1223374393]: [508.366944ms] [508.366944ms] END
I0128 14:18:47.242255       1 trace.go:236] Trace[458228872]: "Update" accept:application/json, */*,audit-id:17d31471-2439-40da-8dda-48a6e32a9dfb,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 14:18:46.704) (total time: 537ms):
Trace[458228872]: ["GuaranteedUpdate etcd3" audit-id:17d31471-2439-40da-8dda-48a6e32a9dfb,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 536ms (14:18:46.705)
Trace[458228872]:  ---"Txn call completed" 517ms (14:18:47.241)]
Trace[458228872]: [537.430503ms] [537.430503ms] END
I0128 14:18:48.382434       1 trace.go:236] Trace[801792558]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:595bbc33-3aac-4a36-8eb1-8c8c80f4fe6f,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (28-Jan-2024 14:18:47.603) (total time: 778ms):
Trace[801792558]: ["GuaranteedUpdate etcd3" audit-id:595bbc33-3aac-4a36-8eb1-8c8c80f4fe6f,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 778ms (14:18:47.603)
Trace[801792558]:  ---"Txn call completed" 776ms (14:18:48.381)]
Trace[801792558]: [778.769722ms] [778.769722ms] END
I0128 14:18:51.630349       1 trace.go:236] Trace[110420240]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:df8bc40a-68f2-4ba4-95fb-68cc36bd74f3,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (28-Jan-2024 14:18:50.975) (total time: 654ms):
Trace[110420240]: ["GuaranteedUpdate etcd3" audit-id:df8bc40a-68f2-4ba4-95fb-68cc36bd74f3,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 654ms (14:18:50.975)
Trace[110420240]:  ---"Txn call completed" 650ms (14:18:51.627)]
Trace[110420240]: [654.220356ms] [654.220356ms] END
I0128 14:18:57.316275       1 trace.go:236] Trace[1293398145]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 14:18:56.282) (total time: 1033ms):
Trace[1293398145]: ---"initial value restored" 299ms (14:18:56.581)
Trace[1293398145]: ---"Transaction prepared" 400ms (14:18:56.982)
Trace[1293398145]: ---"Txn call completed" 333ms (14:18:57.316)
Trace[1293398145]: [1.03329621s] [1.03329621s] END
I0128 14:19:37.261883       1 trace.go:236] Trace[1719842247]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 14:19:36.285) (total time: 976ms):
Trace[1719842247]: ---"initial value restored" 187ms (14:19:36.472)
Trace[1719842247]: ---"Transaction prepared" 184ms (14:19:36.657)
Trace[1719842247]: ---"Txn call completed" 603ms (14:19:37.261)
Trace[1719842247]: [976.596676ms] [976.596676ms] END
I0128 14:20:07.664053       1 trace.go:236] Trace[190327725]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 14:20:06.287) (total time: 1376ms):
Trace[190327725]: ---"initial value restored" 391ms (14:20:06.679)
Trace[190327725]: ---"Transaction prepared" 108ms (14:20:06.787)
Trace[190327725]: ---"Txn call completed" 875ms (14:20:07.663)
Trace[190327725]: [1.376779145s] [1.376779145s] END
I0128 14:20:08.300851       1 trace.go:236] Trace[822513565]: "Update" accept:application/json, */*,audit-id:fce55727-9b58-45d1-88f7-7bea726ac0d4,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 14:20:07.671) (total time: 629ms):
Trace[822513565]: ["GuaranteedUpdate etcd3" audit-id:fce55727-9b58-45d1-88f7-7bea726ac0d4,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 628ms (14:20:07.672)
Trace[822513565]:  ---"Txn call completed" 624ms (14:20:08.300)]
Trace[822513565]: [629.015075ms] [629.015075ms] END

* 
* ==> kube-controller-manager [159ca0905797] <==
* I0125 14:19:21.926389       1 shared_informer.go:318] Caches are synced for endpoint
I0125 14:19:21.926968       1 shared_informer.go:318] Caches are synced for ephemeral
I0125 14:19:21.930037       1 shared_informer.go:318] Caches are synced for PVC protection
I0125 14:19:21.955277       1 shared_informer.go:318] Caches are synced for deployment
I0125 14:19:21.960321       1 shared_informer.go:318] Caches are synced for resource quota
I0125 14:19:21.985371       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0125 14:19:22.006531       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-qtflq"
I0125 14:19:22.018568       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="33.169958ms"
I0125 14:19:22.021642       1 shared_informer.go:318] Caches are synced for resource quota
I0125 14:19:22.030215       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="11.098593ms"
I0125 14:19:22.030424       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="82.452¬µs"
I0125 14:19:22.040459       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="146.375¬µs"
I0125 14:19:22.350230       1 shared_informer.go:318] Caches are synced for garbage collector
I0125 14:19:22.350306       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0125 14:19:22.360431       1 shared_informer.go:318] Caches are synced for garbage collector
I0125 14:19:24.608808       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="235.816¬µs"
I0125 14:20:02.427828       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="35.176972ms"
I0125 14:20:02.428468       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="162.344¬µs"
I0125 14:22:00.161770       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-7fd5cb4ddc to 1"
I0125 14:22:00.172939       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.180159       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-8694d4445c to 1"
I0125 14:22:00.185854       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="24.355471ms"
E0125 14:22:00.185893       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.186110       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.192515       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="5.033629ms"
E0125 14:22:00.192549       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.193417       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.201567       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="21.609317ms"
E0125 14:22:00.201664       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.204711       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="12.121223ms"
E0125 14:22:00.204746       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.205135       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.207577       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="5.816869ms"
E0125 14:22:00.207779       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.207707       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.211031       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="6.232736ms"
E0125 14:22:00.211258       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.211186       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.219261       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="11.43726ms"
E0125 14:22:00.219285       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.219412       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.223396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="4.076586ms"
E0125 14:22:00.223421       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0125 14:22:00.223467       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0125 14:22:00.246934       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-8694d4445c-pdk2b"
I0125 14:22:00.263330       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="23.130452ms"
I0125 14:22:00.271340       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-7fd5cb4ddc-hlswt"
I0125 14:22:00.290467       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="38.408433ms"
I0125 14:22:00.297132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="33.579758ms"
I0125 14:22:00.297351       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="89.086¬µs"
I0125 14:22:00.302313       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="11.586051ms"
I0125 14:22:00.302432       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="68.739¬µs"
I0125 14:22:00.303708       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="53.681¬µs"
I0125 14:24:13.600160       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="10.808714ms"
I0125 14:24:13.600325       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="50.803¬µs"
I0125 14:24:48.254043       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="9.577417ms"
I0125 14:24:48.254416       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="38.475¬µs"
I0125 16:19:08.372276       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-x6kvv" approvedExpiration="1h0m0s"
E0125 18:32:53.930172       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0125 18:32:54.173265       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-controller-manager [a3c291655c2d] <==
* I0128 13:49:08.349513       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 13:49:08.418875       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 13:49:08.443503       1 shared_informer.go:318] Caches are synced for TTL
I0128 13:49:08.444034       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0128 13:49:08.444804       1 shared_informer.go:318] Caches are synced for crt configmap
I0128 13:49:08.445283       1 shared_informer.go:318] Caches are synced for node
I0128 13:49:08.445770       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0128 13:49:08.445941       1 range_allocator.go:174] "Sending events to api server"
I0128 13:49:08.446055       1 range_allocator.go:178] "Starting range CIDR allocator"
I0128 13:49:08.446079       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0128 13:49:08.446105       1 shared_informer.go:318] Caches are synced for cidrallocator
I0128 13:49:08.443512       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0128 13:49:08.455156       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 13:49:08.455470       1 shared_informer.go:318] Caches are synced for PV protection
I0128 13:49:08.459965       1 shared_informer.go:318] Caches are synced for expand
I0128 13:49:08.465200       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0128 13:49:08.552028       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0128 13:49:08.559677       1 shared_informer.go:318] Caches are synced for namespace
I0128 13:49:08.559837       1 shared_informer.go:318] Caches are synced for service account
I0128 13:49:08.559683       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0128 13:49:08.573952       1 shared_informer.go:318] Caches are synced for stateful set
I0128 13:49:08.582825       1 shared_informer.go:318] Caches are synced for ephemeral
I0128 13:49:08.584825       1 shared_informer.go:318] Caches are synced for PVC protection
I0128 13:49:08.584902       1 shared_informer.go:318] Caches are synced for TTL after finished
I0128 13:49:08.584982       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0128 13:49:08.586065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="162.605¬µs"
I0128 13:49:08.592659       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="6.488973ms"
I0128 13:49:08.594317       1 shared_informer.go:318] Caches are synced for ReplicationController
I0128 13:49:08.595260       1 shared_informer.go:318] Caches are synced for disruption
I0128 13:49:08.595870       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0128 13:49:08.597298       1 shared_informer.go:318] Caches are synced for deployment
I0128 13:49:08.622410       1 shared_informer.go:318] Caches are synced for resource quota
I0128 13:49:08.622722       1 shared_informer.go:318] Caches are synced for attach detach
I0128 13:49:08.635274       1 shared_informer.go:318] Caches are synced for HPA
I0128 13:49:08.637271       1 shared_informer.go:318] Caches are synced for job
I0128 13:49:08.637667       1 shared_informer.go:318] Caches are synced for daemon sets
I0128 13:49:08.650617       1 shared_informer.go:318] Caches are synced for cronjob
I0128 13:49:08.671713       1 shared_informer.go:318] Caches are synced for taint
I0128 13:49:08.672039       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0128 13:49:08.672418       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0128 13:49:08.672604       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0128 13:49:08.672653       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0128 13:49:08.672825       1 taint_manager.go:211] "Sending events to api server"
I0128 13:49:08.694835       1 shared_informer.go:318] Caches are synced for resource quota
I0128 13:49:08.698439       1 shared_informer.go:318] Caches are synced for persistent volume
I0128 13:49:08.700560       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0128 13:49:08.700714       1 shared_informer.go:318] Caches are synced for GC
I0128 13:49:08.706200       1 shared_informer.go:318] Caches are synced for endpoint
I0128 13:49:08.857121       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="271.91762ms"
I0128 13:49:08.884109       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="195.474¬µs"
I0128 13:49:08.922427       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 13:49:08.922494       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0128 13:49:08.952642       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 13:49:36.496171       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="42.661953ms"
I0128 13:49:36.496746       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="202.23¬µs"
I0128 13:49:44.315488       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="39.775532ms"
I0128 13:49:44.316399       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="168.203¬µs"
I0128 13:49:44.419118       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="212.111¬µs"
I0128 13:49:59.547858       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="75.031833ms"
I0128 13:49:59.548636       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="480.253¬µs"

* 
* ==> kube-proxy [86a19c6c1599] <==
* I0125 14:19:25.590825       1 server_others.go:69] "Using iptables proxy"
I0125 14:19:25.763484       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0125 14:19:25.839116       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0125 14:19:25.850593       1 server_others.go:152] "Using iptables Proxier"
I0125 14:19:25.850772       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0125 14:19:25.850826       1 server_others.go:438] "Defaulting to no-op detect-local"
I0125 14:19:25.857624       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0125 14:19:25.858602       1 server.go:846] "Version info" version="v1.28.3"
I0125 14:19:25.858654       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0125 14:19:25.881004       1 config.go:188] "Starting service config controller"
I0125 14:19:25.881717       1 config.go:97] "Starting endpoint slice config controller"
I0125 14:19:25.883120       1 config.go:315] "Starting node config controller"
I0125 14:19:25.886372       1 shared_informer.go:311] Waiting for caches to sync for node config
I0125 14:19:25.885159       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0125 14:19:25.885948       1 shared_informer.go:311] Waiting for caches to sync for service config
I0125 14:19:25.987935       1 shared_informer.go:318] Caches are synced for node config
I0125 14:19:25.989039       1 shared_informer.go:318] Caches are synced for service config
I0125 14:19:25.989149       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [fbbe09bad200] <==
* I0128 13:49:03.955071       1 server_others.go:69] "Using iptables proxy"
I0128 13:49:04.622964       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0128 13:49:05.374893       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0128 13:49:05.389410       1 server_others.go:152] "Using iptables Proxier"
I0128 13:49:05.389530       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0128 13:49:05.389565       1 server_others.go:438] "Defaulting to no-op detect-local"
I0128 13:49:05.389791       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0128 13:49:05.392118       1 server.go:846] "Version info" version="v1.28.3"
I0128 13:49:05.392166       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 13:49:05.444918       1 config.go:188] "Starting service config controller"
I0128 13:49:05.445636       1 shared_informer.go:311] Waiting for caches to sync for service config
I0128 13:49:05.447539       1 config.go:97] "Starting endpoint slice config controller"
I0128 13:49:05.448117       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0128 13:49:05.457477       1 config.go:315] "Starting node config controller"
I0128 13:49:05.457564       1 shared_informer.go:311] Waiting for caches to sync for node config
I0128 13:49:05.546953       1 shared_informer.go:318] Caches are synced for service config
I0128 13:49:05.551338       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0128 13:49:05.557852       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [7effe5b7db86] <==
* I0128 13:48:45.578199       1 serving.go:348] Generated self-signed cert in-memory
W0128 13:48:53.665958       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 13:48:53.666264       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 13:48:53.666421       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 13:48:53.666577       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 13:48:53.811708       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0128 13:48:53.811779       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 13:48:53.816274       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 13:48:53.816368       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 13:48:53.820803       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0128 13:48:53.821039       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0128 13:48:53.930450       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [9045a5d0b388] <==
* I0125 14:19:04.742639       1 serving.go:348] Generated self-signed cert in-memory
W0125 14:19:05.355476       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0125 14:19:05.355538       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0125 14:19:05.355593       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0125 14:19:05.355644       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0125 14:19:05.399765       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0125 14:19:05.399827       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0125 14:19:05.406575       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0125 14:19:05.406690       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0125 14:19:05.410070       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0125 14:19:05.412227       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0125 14:19:05.421977       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0125 14:19:05.422147       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0125 14:19:05.423947       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0125 14:19:05.424013       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0125 14:19:05.424195       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0125 14:19:05.424241       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0125 14:19:05.424425       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0125 14:19:05.424460       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0125 14:19:05.424597       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0125 14:19:05.424631       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0125 14:19:05.427671       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0125 14:19:05.427737       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0125 14:19:05.427868       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0125 14:19:05.427907       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0125 14:19:05.427952       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0125 14:19:05.427984       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0125 14:19:05.427953       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0125 14:19:05.428053       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0125 14:19:05.428088       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0125 14:19:05.428124       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0125 14:19:05.428140       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0125 14:19:05.428173       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0125 14:19:05.428205       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0125 14:19:05.428239       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0125 14:19:05.428279       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0125 14:19:05.428314       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0125 14:19:05.428861       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0125 14:19:05.428901       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0125 14:19:05.431911       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0125 14:19:05.431957       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0125 14:19:06.227305       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0125 14:19:06.227346       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0125 14:19:06.236975       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0125 14:19:06.237015       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
I0125 14:19:08.407109       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jan 28 13:48:35 minikube kubelet[1495]: E0128 13:48:35.351059    1495 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="1.6s"
Jan 28 13:48:35 minikube kubelet[1495]: I0128 13:48:35.747165    1495 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 28 13:48:35 minikube kubelet[1495]: E0128 13:48:35.749531    1495 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jan 28 13:48:36 minikube kubelet[1495]: E0128 13:48:36.953494    1495 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="3.2s"
Jan 28 13:48:37 minikube kubelet[1495]: I0128 13:48:37.067530    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="35693f60d5223b4286af2f23c9f14a629b09e0c3aef48297eccabc4911c6299c"
Jan 28 13:48:37 minikube kubelet[1495]: I0128 13:48:37.103414    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fa6850b03965c835eb46fe7c352d21262c7733d5b23a0e461026dfab4501b9fc"
Jan 28 13:48:37 minikube kubelet[1495]: W0128 13:48:37.188941    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: E0128 13:48:37.189127    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: W0128 13:48:37.227824    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: E0128 13:48:37.227998    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: W0128 13:48:37.377076    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: E0128 13:48:37.377274    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: I0128 13:48:37.407415    1495 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 28 13:48:37 minikube kubelet[1495]: E0128 13:48:37.408418    1495 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jan 28 13:48:37 minikube kubelet[1495]: W0128 13:48:37.646924    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:37 minikube kubelet[1495]: E0128 13:48:37.647175    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:40 minikube kubelet[1495]: E0128 13:48:40.155829    1495 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="6.4s"
Jan 28 13:48:40 minikube kubelet[1495]: I0128 13:48:40.649906    1495 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 28 13:48:40 minikube kubelet[1495]: E0128 13:48:40.651177    1495 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jan 28 13:48:41 minikube kubelet[1495]: W0128 13:48:41.053979    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:41 minikube kubelet[1495]: E0128 13:48:41.055381    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:41 minikube kubelet[1495]: W0128 13:48:41.912569    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:41 minikube kubelet[1495]: E0128 13:48:41.912820    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:42 minikube kubelet[1495]: W0128 13:48:42.133675    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:42 minikube kubelet[1495]: E0128 13:48:42.139017    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:43 minikube kubelet[1495]: W0128 13:48:43.007299    1495 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:43 minikube kubelet[1495]: E0128 13:48:43.007486    1495 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jan 28 13:48:43 minikube kubelet[1495]: E0128 13:48:43.731956    1495 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube.17ae86f9403bccb9", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube", UID:"minikube", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2024, time.January, 28, 13, 48, 33, 902636217, time.Local), LastTimestamp:time.Date(2024, time.January, 28, 13, 48, 33, 902636217, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"minikube"}': 'Post "https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events": dial tcp 192.168.49.2:8443: connect: connection refused'(may retry after sleeping)
Jan 28 13:48:44 minikube kubelet[1495]: E0128 13:48:44.605555    1495 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Jan 28 13:48:47 minikube kubelet[1495]: I0128 13:48:47.153535    1495 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.846344    1495 apiserver.go:52] "Watching apiserver"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.917750    1495 topology_manager.go:215] "Topology Admit Handler" podUID="a939272e-11d3-4775-bd6f-0e6573e7403a" podNamespace="kube-system" podName="storage-provisioner"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.935588    1495 topology_manager.go:215] "Topology Admit Handler" podUID="37588c65-57ab-4237-9b2f-30ff380ae1ec" podNamespace="kube-system" podName="kube-proxy-6w7wj"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.936261    1495 topology_manager.go:215] "Topology Admit Handler" podUID="8ffd43dc-6f9b-4b14-93c5-401cc05acf60" podNamespace="kube-system" podName="coredns-5dd5756b68-qtflq"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.936873    1495 topology_manager.go:215] "Topology Admit Handler" podUID="2100e417-fb0d-4d02-912a-40e98fd2bff5" podNamespace="kubernetes-dashboard" podName="dashboard-metrics-scraper-7fd5cb4ddc-hlswt"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.937516    1495 topology_manager.go:215] "Topology Admit Handler" podUID="4262be9d-4617-4578-827c-d7b49346b44a" podNamespace="kubernetes-dashboard" podName="kubernetes-dashboard-8694d4445c-pdk2b"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.967018    1495 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.967305    1495 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.991588    1495 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 13:48:53 minikube kubelet[1495]: I0128 13:48:53.997191    1495 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 28 13:48:54 minikube kubelet[1495]: I0128 13:48:54.005389    1495 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a939272e-11d3-4775-bd6f-0e6573e7403a-tmp\") pod \"storage-provisioner\" (UID: \"a939272e-11d3-4775-bd6f-0e6573e7403a\") " pod="kube-system/storage-provisioner"
Jan 28 13:48:54 minikube kubelet[1495]: E0128 13:48:54.015243    1495 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jan 28 13:48:54 minikube kubelet[1495]: I0128 13:48:54.032346    1495 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Jan 28 13:48:54 minikube kubelet[1495]: I0128 13:48:54.110691    1495 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/37588c65-57ab-4237-9b2f-30ff380ae1ec-xtables-lock\") pod \"kube-proxy-6w7wj\" (UID: \"37588c65-57ab-4237-9b2f-30ff380ae1ec\") " pod="kube-system/kube-proxy-6w7wj"
Jan 28 13:48:54 minikube kubelet[1495]: I0128 13:48:54.111768    1495 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/37588c65-57ab-4237-9b2f-30ff380ae1ec-lib-modules\") pod \"kube-proxy-6w7wj\" (UID: \"37588c65-57ab-4237-9b2f-30ff380ae1ec\") " pod="kube-system/kube-proxy-6w7wj"
Jan 28 13:48:57 minikube kubelet[1495]: I0128 13:48:57.450882    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a17b7e002dc60037c4132af51487fcdd47de61e7a8a4b34f24312681f9292d59"
Jan 28 13:48:57 minikube kubelet[1495]: I0128 13:48:57.664484    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0dfec6008e4b183e3056736909cc204f8a4407d26af52f471317dc6acffd8dfa"
Jan 28 13:48:57 minikube kubelet[1495]: I0128 13:48:57.720798    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f6ce4e97d0302a1a95990aa59dabe6d0d3e504b7e9cf9d3064b0a360cbda4585"
Jan 28 13:48:57 minikube kubelet[1495]: I0128 13:48:57.840402    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="045929d139580f7cdd3fbf4b10951be296722dd68817a7c8e9722c761913ddb5"
Jan 28 13:48:57 minikube kubelet[1495]: I0128 13:48:57.938594    1495 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="98850f4e4104ae6797704c6ae856be432872a6e57d0c1cefe3b4cd726620291f"
Jan 28 13:49:31 minikube kubelet[1495]: I0128 13:49:31.058682    1495 scope.go:117] "RemoveContainer" containerID="171733a855a932edbe4d20ee4db40d5cfe537a2d05e1b72d66e0b9213dcb0576"
Jan 28 13:49:31 minikube kubelet[1495]: I0128 13:49:31.065497    1495 scope.go:117] "RemoveContainer" containerID="9137ed4b79c3544bd711a83a2433890d036298b1e9ddbabdc4cf2983130a4f2f"
Jan 28 13:49:31 minikube kubelet[1495]: E0128 13:49:31.067379    1495 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(a939272e-11d3-4775-bd6f-0e6573e7403a)\"" pod="kube-system/storage-provisioner" podUID="a939272e-11d3-4775-bd6f-0e6573e7403a"
Jan 28 13:49:36 minikube kubelet[1495]: I0128 13:49:36.373727    1495 scope.go:117] "RemoveContainer" containerID="ee3891153d106c88531c9cab9f56a143b0502ac9a9efd4d3d857079fb92cc11b"
Jan 28 13:49:36 minikube kubelet[1495]: I0128 13:49:36.375183    1495 scope.go:117] "RemoveContainer" containerID="a33fb1c4f6164387fbb5bf40cdea493cb25bb8fe6809f44291fe7d21a04b10e9"
Jan 28 13:49:36 minikube kubelet[1495]: E0128 13:49:36.376718    1495 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-pdk2b_kubernetes-dashboard(4262be9d-4617-4578-827c-d7b49346b44a)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-pdk2b" podUID="4262be9d-4617-4578-827c-d7b49346b44a"
Jan 28 13:49:42 minikube kubelet[1495]: I0128 13:49:42.158541    1495 scope.go:117] "RemoveContainer" containerID="9137ed4b79c3544bd711a83a2433890d036298b1e9ddbabdc4cf2983130a4f2f"
Jan 28 13:49:44 minikube kubelet[1495]: I0128 13:49:44.375654    1495 scope.go:117] "RemoveContainer" containerID="a33fb1c4f6164387fbb5bf40cdea493cb25bb8fe6809f44291fe7d21a04b10e9"
Jan 28 13:49:44 minikube kubelet[1495]: E0128 13:49:44.377714    1495 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-pdk2b_kubernetes-dashboard(4262be9d-4617-4578-827c-d7b49346b44a)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-pdk2b" podUID="4262be9d-4617-4578-827c-d7b49346b44a"
Jan 28 13:49:58 minikube kubelet[1495]: I0128 13:49:58.159394    1495 scope.go:117] "RemoveContainer" containerID="a33fb1c4f6164387fbb5bf40cdea493cb25bb8fe6809f44291fe7d21a04b10e9"

* 
* ==> kubernetes-dashboard [5e938fde846b] <==
* 2024/01/28 13:49:59 Starting overwatch
2024/01/28 13:49:59 Using namespace: kubernetes-dashboard
2024/01/28 13:49:59 Using in-cluster config to connect to apiserver
2024/01/28 13:49:59 Using secret token for csrf signing
2024/01/28 13:49:59 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/01/28 13:49:59 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/01/28 13:49:59 Successful initial request to the apiserver, version: v1.28.3
2024/01/28 13:49:59 Generating JWE encryption key
2024/01/28 13:49:59 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/01/28 13:49:59 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/01/28 13:50:00 Initializing JWE encryption key from synchronized object
2024/01/28 13:50:00 Creating in-cluster Sidecar client
2024/01/28 13:50:00 Successful request to sidecar
2024/01/28 13:50:00 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [a33fb1c4f616] <==
* 2024/01/28 13:49:05 Using namespace: kubernetes-dashboard
2024/01/28 13:49:05 Using in-cluster config to connect to apiserver
2024/01/28 13:49:05 Using secret token for csrf signing
2024/01/28 13:49:05 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/01/28 13:49:05 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00053fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0001b4a80)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [9137ed4b79c3] <==
* I0128 13:49:00.214411       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0128 13:49:30.553228       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [c02f0c52dff1] <==
* I0128 13:49:42.686680       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0128 13:49:42.744369       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0128 13:49:42.746811       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0128 13:50:00.214889       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0128 13:50:00.215416       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e6a50d9b-a494-45e9-a342-79688b89168b!
I0128 13:50:00.245450       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"c5e543da-f024-4dfa-a8de-fa99c655163c", APIVersion:"v1", ResourceVersion:"12289", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e6a50d9b-a494-45e9-a342-79688b89168b became leader
I0128 13:50:00.416950       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e6a50d9b-a494-45e9-a342-79688b89168b!

